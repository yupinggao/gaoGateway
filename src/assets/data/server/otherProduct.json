[
  {
    "id": 200,
    "date": "2019/01/03",
    "title": "Jenkins",
    "itemList": [
      {
        "text": "<p>Jenkins is an open source automation server written in Java. Jenkins helps to automate the non-human part of the software development process, with continuous integration and facilitating technical aspects of continuous delivery. It is a server-based system that runs in servlet containers such as Apache Tomcat. It supports version control tools, including AccuRev, CVS, Subversion, Git, Mercurial, Perforce, TD/OMS, ClearCase and RTC, and can execute Apache Ant, Apache Maven and sbt based projects as well as arbitrary shell scripts and Windows batch commands.</p><p>Jenkins can be installed through native system packages, Docker, or even run standalone by any machine with a Java Runtime Environment (JRE) installed.</p>",
        "image": ""
      },
      {
        "text": "<p>Jenkins initial admin password is stored in <code>/var/lib/jenkins/secrets/initialAdminPassword</code> on Linux.</p>",
        "image": ""
      },
      {
        "text": "<p>To check Jenkins status on Linux: <code>service jenkins status</code></p>",
        "image": ""
      },
      {
        "text": "Builds can be triggered by various means, for example by commit in a version control system, by scheduling via a cron-like mechanism and by requesting a specific build URL. It can also be triggered after the other builds in the queue have completed. Jenkins functionality can be extended with plugins.",
        "image": ""
      },
      {
        "text": "<b>Jenkins Pipeline</b><br/><p>Jenkins Pipeline (or simply 'Pipeline') is a suite of plugins which supports implementing and integrating continuous delivery pipelines into Jenkins.<p>A continuous delivery pipeline is an automated expression of your process for getting software from version control right through to your users and customers.</p><p>Jenkins Pipeline provides an extensible set of tools for modeling simple-to-complex delivery pipelines 'as code'. The definition of a Jenkins Pipeline is typically written into a text file (called a Jenkinsfile) which in turn is checked into a project’s source control repository.</p><p>Figure below shows a diagram of all the sections you can have in a Declarative Pipeline. The way to read this chart is that items with solid line borders are required, and items with dotted line borders are optional:</p>",
        "image": "../assets/image/it/tool/jenkinsPipeline.png",
        "imageClass": "mx-auto d-block"
      }
    ]
  },
  {
    "id": 300,
    "date": "2019/01/06",
    "title": "Gira",
    "itemList": [
      {
        "text": "<p>Jira is a proprietary issue tracking product developed by Atlassian that allows bug tracking and agile project management.</p>",
        "image": ""
      },
      {
        "text": "<p>For remote procedure calls (RPC), Jira supports REST, SOAP, and XML-RPC. Jira integrates with source control programs such as Clearcase, Concurrent Versions System (CVS), Git, Mercurial, Perforce, Subversion, and Team Foundation Server.</p>",
        "image": ""
      }
    ]
  },
  {
    "id": 400,
    "date": "2019/03/17",
    "title": "Dataweave",
    "itemList": [
      {
        "text": "<p><b>DataWeave Types</b><br/>DataWeave functions operate on data of many types, including <ul><li>Arrays</li><li>Booleans</li><li>CData</li><li>Date and Time</li><li>Enum</li><li>Iterator</li><li>Number</li><li>Object</li><li>Regex</li><li>String</li><li>TryResult</li></ul>The types that DataWeave provide are bundled into modules that also contain the related functions.</p>",
        "image": ""
      },
      {
        "text": "",
        "image": ""
      }
    ]
  },
  {
    "id": 500,
    "date": "2020/03/10",
    "title": "Autosys",
    "itemList": [
      {
        "text": "<p>Autosys is used to start Java process, takes backup of log files, stop Java process, cleaning and purging database and will all sort of housekeeping jobs in Linux environment. Autosys system is made of Autosys server and Autosys clients, each server or box, which has services scheduled by autosys, requires autosys client to be install on that. One of the key advantages of Autosys is that one job can depend upon another job, and can execute, depending upon, success and failure of parent job. Though Java developers are not asked a lot of interview questions, you might expect few of them during your interview with Wall Street banks, Brokers or Financial Institution, which uses autosys.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Autosys Job Status</b></p><p><ul><li>STARTING</li><li>RUNNING</li><li>INACTIVE</li><li>ACTIVATED</li><li>SUCCESS</li><li>FAILURE</li><li>TERMINATED</li><li>RESTART</li><li>QUE_WAIT</li><li>ON HOLD</li><li>ON ICE</li></ul></p>",
        "image": "../assets/image/it/tool/onHoldvsOnIce.png",
        "imageClass": "mx-auto d-block"
      },
      {
        "text": "<p><b>Commands</b></p><p><ul><li><code>sendevent -E STARTJOB -J <i>job_name</i></code></li></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 600,
    "date": "2019/11/30",
    "title": "Api Gateway",
    "itemList": [
      {
        "text": "<p>An API Gateway is a server that is the single entry point into the system. It is similar to the Facade pattern from object-oriented design. The API Gateway encapsulates the internal system architecture and provides an API that is tailored to each client. It might have other responsibilities such as authentication, monitoring, load balancing, caching, request shaping and management, and static response handling.</p><p>The following diagram shows how an API Gateway typically fits into the architecture:</p>",
        "image": "../assets/image/server/web/gateway.png",
        "imageHeight": "450",
        "imageWidth": "800"
      },
      {
        "text": "<p>A gateway helps to address these issues by decoupling clients from services. Gateways can perform a number of different functions, and you may not need all of them. The functions can be grouped into the following design patterns:</p><p>Gateway Routing. Use the gateway as a reverse proxy to route requests to one or more backend services, using layer 7 routing. The gateway provides a single endpoint for clients, and helps to decouple clients from services.</p><p>Gateway Aggregation. Use the gateway to aggregate multiple individual requests into a single request. This pattern applies when a single operation requires calls to multiple backend services. The client sends one request to the gateway. The gateway dispatches requests to the various backend services, and then aggregates the results and sends them back to the client. This helps to reduce chattiness between the client and the backend.</p><p>Gateway Offloading. Use the gateway to offload functionality from individual services to the gateway, particularly cross-cutting concerns. It can be useful to consolidate these functions into one place, rather than making every service responsible for implementing them. This is particularly true for features that requires specialized skills to implement correctly, such as authentication and authorization.</p><p>Here are some examples of functionality that could be offloaded to a gateway:<ul><li>SSL termination</li><li>Authentication</li><li>IP whitelisting</li><li>Client rate limiting (throttling)</li><li>Logging and monitoring</li><li>Response caching</li><li>Web application firewall</li><li>GZIP compression</li><li>Servicing static content</li></ul></p>",
        "image": ""
      },
      {
        "text": "<b>Choosing a gateway technology</b><p>Here are some options for implementing an API gateway in your application.<ul><li><b>Reverse proxy server</b>. Nginx and HAProxy are popular reverse proxy servers that support features such as load balancing, SSL, and layer 7 routing. They are both free, open-source products, with paid editions that provide additional features and support options. Nginx and HAProxy are both mature products with rich feature sets and high performance. You can extend them with third-party modules or by writing custom scripts in Lua. Nginx also supports a JavaScript-based scripting module called NginScript.</li><li><b>Service mesh ingress controller</b>. If you are using a service mesh such as linkerd or Istio, consider the features that are provided by the ingress controller for that service mesh. For example, the Istio ingress controller supports layer 7 routing, HTTP redirects, retries, and other features.</li><li><b>Azure Application Gateway</b>. Application Gateway is a managed load balancing service that can perform layer-7 routing and SSL termination. It also provides a web application firewall (WAF). Application Gateway and Web Application Firewall (WAF) are also available under a Standard_v2 and WAF_v2 SKU. The v2 SKU offers performance enhancements and adds support for critical new features like autoscaling, zone redundancy, and support for static VIPs. Existing features under the Standard and WAF SKU continue to be supported in the new v2 SKU. The new v2 SKU includes the following enhancements:<ul><li>Autoscaling</li><li>Zone redundancy</li><li>Static VIP</li><li>Header Rewrite</li><li>Key Vault Integration</li><li>Azure Kubernetes Service Ingress Controller</li><li>Performance enhancements</li><li>Faster deployment and update time</li></ul></li><li><b>Azure API Management</b>. API Management is a turnkey solution for publishing APIs to external and internal customers. It provides features that are useful for managing a public-facing API, including rate limiting, IP white listing, and authentication using Azure Active Directory or other identity providers. API Management doesn't perform any load balancing, so it should be used in conjunction with a load balancer such as Application Gateway or a reverse proxy. For information about using API Management with Application Gateway, see Integrate API Management in an internal VNet with Application Gateway.</li></ul></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 700,
    "date": "2020/07/09",
    "title": "RabbitMQ",
    "itemList": [
      {
        "text": "<p>With tens of thousands of users, RabbitMQ is one of the most popular open source message brokers. RabbitMQ is lightweight and easy to deploy on premises and in the cloud. It supports multiple messaging protocols. RabbitMQ can be deployed in distributed and federated configurations to meet high-scale, high-availability requirements.</p>",
        "image": ""
      },
      {
        "text": "<p>use <code>rabbitmq-plugins enable rabbitmq_management</code> to enable UI</p>",
        "image": ""
      },
      {
        "text": "<p>use <code>rabbitmqctl status</code> to get status</p>",
        "image": ""
      },
      {
        "text": "<p>connect to RabbitMQ UI using http://localhost:15672/ (default user/password: guest/guest)</p>",
        "image": ""
      }
    ]
  },
  {
    "id": 800,
    "date": "2020/07/10",
    "title": "Kafka",
    "itemList": [
      {
        "text": "<p>Apache Kafka® is a distributed streaming platform. It has three key capabilities:<ul><li>Publish and subscribe to streams of records, similar to a message queue or enterprise messaging system.</li><li>Store streams of records in a fault-tolerant durable way.</li><li>Process streams of records as they occur.</li></ul></p>",
        "image": "../assets/image/server/otherProduct/kafka.PNG",
        "imageClass": "mx-auto d-block"
      },
      {
        "text": "<p>Change to kafka base directory, and use <code>.\\bin\\windows\\zookeeper-server-start.bat .\\config\\zookeeper.properties</code> to start the zookeeper</p>",
        "image": ""
      },
      {
        "text": "<p>Change to kafka base directory, and use <code>.\\bin\\windows\\kafka-server-start.bat .\\config\\server.properties</code> to start the kafka server</p>",
        "image": ""
      }
    ]
  },
  {
    "id": 900,
    "date": "2020/07/15",
    "title": "Informatica",
    "itemList": [
      {
        "text": "<p>INFORMATICA is a Software development company, which offers data integration products. It offers products for ETL, data masking, data Quality, data replica, data virtualization, master data management, etc. Informatica Powercenter ETL/Data Integration tool is the most widely used tool and in the common term when we say Informatica, it refers to the Informatica PowerCenter tool for ETL.</p>",
        "image": "../assets/image/server/otherProduct/informaticaBI.PNG",
        "imageClass": "mx-auto d-block"
      },
      {
        "text": "<p>Informatica Powercenter is used for Data integration. It offers the capability to connect & fetch data from different heterogeneous source and processing of data. For example, you can connect to an SQL Server Database and Oracle Database both and can integrate the data into a third system.</p>",
        "image": "../assets/image/server/otherProduct/informaticaPowerCenter.PNG",
        "imageClass": "mx-auto d-block"
      },
      {
        "text": "<p>INFORMATICA Powercenter has four components: Repository Manager, DEsigner, Workflow Manager, Workflow Monitor.</p>",
        "image": "../assets/image/server/otherProduct/informatica.PNG",
        "imageClass": "mx-auto d-block"
      }
    ]
  },
  {
    "id": 1000,
    "date": "2020/10/20",
    "title": "Mulesoft",
    "itemList": [
      {
        "text": "<p>A Mule event contains the core information processed by the runtime. It travels through components inside your Mule app following the configured application logic.</p><p>Note that the Mule event is immutable, so every change to an instance of a Mule event results in the creation of a new instance.</p><p>A Mule event is generated when a trigger (such as an HTTP request or a change to a database or file) reaches the Event source of a flow. This trigger could be an external event triggered by a resource that might be external to the Mule app.</p>",
        "image": "../assets/image/server/otherProduct/mulesoftEvent.PNG",
        "imageClass": "mx-auto d-block"
      }
    ]
  },
  {
    "id": 1200,
    "date": "2020/10/20",
    "title": "Anypoint Platform CLI",
    "itemList": [
      {
        "text": "<p>Prerequisites:<ul><li>NodeJS and npm</li><li>Git</li><ul></p>",
        "image": ""
      },
      {
        "text": "<p>Installation: <code>npm install -g anypoint-cli@latest</code></p>",
        "image": ""
      },
      {
        "text": "<p>Usage: <code>anypoint-cli --username yupinggaogmail</code></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 1300,
    "date": "2020/10/30",
    "title": "Salesforce",
    "itemList": [
      {
        "text": "<p><b>Salesforce Architecture</b><br>When you think about the Salesforce architecture, imagine a series of layers that sit on top of each other. Sometimes it helps to think of it as a cake because cake is delicious, and it makes everything better.</p>",
        "image": "../assets/image/server/otherProduct/salesforceArchitecture.PNG",
        "imageClass": "mx-auto d-block"
      },
      {
        "text": "<p><b>There’s a lot to unpack here, but let’s focus on the most important points.</b><ol><li>Salesforce is a cloud company. Everything we offer resides in the trusted, multitenant cloud.</li><li>The Salesforce platform is the foundation of our services. It’s powered by metadata and made up of different parts, like data services, artificial intelligence, and robust APIs for development.</li><li>All our apps sit on top of the platform. Our prebuilt offerings like Sales Cloud and Marketing Cloud, along with apps you build using the platform, have consistent, powerful functionality.</li><li>Everything is integrated. Our platform technologies like Einstein predictive intelligence and the Lightning framework for development are built into everything we offer and everything you build.</li></ol></p>",
        "image": ""
      },
      {
        "text": "<p><b>Differences Between Salesforce Classic vs Lightning</b><ol><li><b>Enhanced User Experience:</b> One of the major differences between Salesforce Classic and Salesforce Lightning is the user interface. The latter offers a much better user interface experience to its users that includes features like the drag-and-drop functionality that can be achieved without any code. Instead of hiring a Salesforce developer to create a Salesforce page or modify it, the page components can easily be rearranged by an admin according to their liking. <br>Furthermore, Lightning helps in toning down the need for Visualforce for every task. The codes that are created during any kind of development need to be tested and then deployed. In case of missed bugs, the code is sent back to the developer to fix and the process starts over again. But with the help of Salesforce Lightning, these types of tedious processes can be avoided. <br>Lightning does not mean that companies don’t need Visualforce developers anymore. Lightning provides a helping hand to the developers by moving minor customization tasks to the admins, which allows developers to focus on larger Salesforce app development projects.</li><li><b>Higher Security:</b> With Salesforce Lightning comes enhanced security. For instance, LockerService is a feature that separates Lightning components for them to interact with each other. This helps in safeguarding the platform from malicious data. No such feature can be found in the Classic mode.<br>Permissions, too, work quite distinctly in Salesforce Lightning. The platform does not allow users to raise their assurance levels, say from standard to high, in-session. For that, they will have to log out of the Lightning platform and sign in again with authentication with a higher assurance level.</li><li><b>Einstein (Wave) Analytics:</b> While an enhanced and upgraded user interface and security are the points strong enough for the comparison, Salesforce Lightning also provides users with access to Einstein (Wave) Analytics reporting, which the Classic does not. For creating graphs, charts, and lists, the Salesforce Classic reports depend on standard reporting types. At the time the data is refreshed, these dashboards prove to be a great option for capturing a view of important metrics.<br>When we talk about Einstein Analytics, it is a whole different deal. The platform carries its own database that is fetched from Salesforce and updates each hour automatically. Also, the dashboard displays the most recent, refreshed data.</li><li><b>Progressive platform:</b> In its initial days, Lightning was looked down at because of its transition and compatibility issues with objects, custom code, and apps. But gone are those days, and the platform has evolved with the ability to support all custom metadata objects, making it a lot simpler for companies to transition their existing apps and workflows with no requirements of building from scratch.</li><li><b>Hassle-free Lead Generation: </b>Although Salesforce Classic enables you to create leads, the Lightning Experience offers more components in order to manage sales processes better. For instance, the Activity timeline in the Lightning mode allows users to identify what’s been achieved for a specific lead and presents the details of every meeting, task, or call. The Path component enables the tracking of various stages involved in the business process, whereas, the News component offers updates about the leads on time.</li></ol></p>",
        "image": ""
      },
      {
        "text": "<p><b>Aura Components</b><br>Aura components are the self-contained and reusable units of an app. They represent a reusable section of the UI, and can range in granularity from a single line of text to an entire app.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Database in Context of Salesforce</b><br>When we talk about the database, think of a giant spreadsheet. When you put information into Salesforce, it gets stored in the database so you can access it again later. It’s stored in a very specific way so you’re always accessing the information you need.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Metadata</b><br>When you look at data in Salesforce, you might think that you're looking at a user interface sitting on top of a run-of-the-mill relational database. But what you’re actually looking at is an abstraction of the database that’s driven by the platform’s metadata-aware architecture.</p><p>In this abstraction, objects are our database tables. The fields on those objects are columns, and records are rows in the database. This analogy is true both for standard objects that come with Salesforce by default and custom objects that you build yourself.</p><p>In short, metadata forms the structure of your org. Whether you’re defining fields, business processes, or something more complex, metadata holds your configuration. The platform then renders your app’s metadata in the user interface along with its associated data.</p>",
        "image": ""
      },
      {
        "text": "<p><b>No-Code and Low-Code Development</b><br>It can be surprising to hear, but the Salesforce platform encourages you to minimize code. And it’s not because we don’t love code. It’s because the platform’s metadata-driven architecture lets you complete most basic development tasks without ever writing a line.</p><p>Salesforce offers a host of tools for point-and-click—or declarative—development. Most of these tools require little to no understanding of development principles: no code.</p><p>Some development tasks, like writing validation rules or hooking up components with UI elements, are considered low code. That means they require some basic programmatic knowledge to complete, but aren’t so rigorous that they’re considered programmatic. For example, if you know something about logic, conditions, and CRUD operations, you can do more with Process Builder.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Salesforce Security</b><br>The platform makes it easy to specify which users can view, create, edit, or delete any record or field in the app. You can control access to your whole org, a specific object, a specific field, or even an individual record. By combining security controls at different levels, you can provide just the right level of data access to thousands of users without having to specify permissions for each user individually.</p>",
        "image": "../assets/image/server/otherProduct/salesforceSecurity.PNG",
        "imageClass": "mx-auto d-block"
      }
    ]
  },
  {
    "id": 1400,
    "date": "2020/11/22",
    "title": "Salesforce Tools",
    "itemList": [
      {
        "text": "<p><b>Automation Tools</b><p>One of the hardest things for an admin or a developer to figure out is when to use what tool for the job at hand. In general, it’s best to start with declarative, no-code tools and work your way up to code solutions.</p><ul><li><b>Process Builder:</b> Use Process Builder when you need to start a behind-the-scenes business process automatically. Processes can start when:<br><ul><li>A record is created</li><li>A record is updated</li><li>A platform event occurs</li></ul></li><li><b>Flow Builder:</b> Use Flow Builder to:<ul><li>Automate a guided visual experience</li><li>Add more functionality for a behind-the-scenes process than is available in Process Builder. Use Flow Builder to build the more complex functionality. Then call the resulting flow from the process</li><li>Start a behind-the-scenes business process:<ul><li>When a user clicks something, like a button</li><li>When a record is created</li><li>When a record is updated</li><li>When a platform event occurs</li><li>At a specified time and frequency</li></ul></li></ul></li><li><b>Approvals:</b> Approvals isn’t included in Lightning Flow, but it offers a declarative way to automate something that Lightning Flow doesn’t cover. That said, Lightning Flow does support automating how a record gets submitted for approval.</li></ul></p>",
        "image": ""
      },
      {
        "text": "<p><b>Which Automation Tool Is Right for My Use Case?</b><br>When it’s all said and done, a process-driven experience isn’t backed by only one process. It’s a combination of all the business processes in your org that can impact your customer. Each business process typically falls into one of these camps.</p>",
        "image": "../assets/image/server/otherProduct/tools.PNG",
        "imageClass": "mx-auto d-block"
      }
    ]
  },
  {
    "id": 1500,
    "date": "2020/11/24",
    "title": "Salesforce Data Security",
    "itemList": [
      {
        "text": "<p><b>Levels of Data Access</b><p>You can control which users have access to which data in your whole org, a specific object, a specific field, or an individual record.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Organization</b><p>For your whole org, you can maintain a list of authorized users, set password policies, and limit logins to certain hours and locations.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Objects</b><p>Access to object-level data is the simplest thing to control. By setting permissions on a particular type of object, you can prevent a group of users from creating, viewing, editing, or deleting any records of that object. For example, you can use object permissions to ensure that interviewers can view positions and job applications but not edit or delete them.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Fields</b><p>You can restrict access to certain fields, even if a user has access to the object. For example, you can make the salary field in a position object invisible to interviewers but visible to hiring managers and recruiters.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Records</b><p>You can allow particular users to view an object, but then restrict the individual object records they're allowed to see. For example, an interviewer can see and edit her own reviews, but not the reviews of other interviewers. You can manage record-level access in these four ways.</p>",
        "image": ""
      },
      {
        "text": "<p><ul><li><b>Organization-wide defaults</b> specify the default level of access users have to each others’ records. You use org-wide sharing settings to lock down your data to the most restrictive level, and then use the other record-level security and sharing tools to selectively give access to other users.</li><li><b>Role hierarchies</b> give access for users higher in the hierarchy to all records owned by users below them in the hierarchy. Role hierarchies don’t have to match your organization chart exactly. Instead, each role in the hierarchy should represent a level of data access that a user or group of users needs.</li><li><b>Sharing rules</b> are automatic exceptions to organization-wide defaults for particular groups of users, so they can get to records they don’t own or can’t normally see. Sharing rules, like role hierarchies, are only used to give additional users access to records. They can’t be stricter than your organization-wide default settings.</li><li><b>Manual sharing</b> allows owners of particular records to share them with other users. Although manual sharing isn’t automated like org-wide sharing settings, role hierarchies, or sharing rules, it can be useful in some situations, such as when a recruiter going on vacation needs to temporarily assign ownership of a job application to someone else.</li></ul></p>",
        "image": ""
      },
      {
        "text": "<p><b>Manage Object Permissions</b><p>The simplest way to control data access is to set permissions on a particular type of object. (An object is a collection of records, like leads or contacts.) You can control whether a group of users can create, view, edit, or delete any records of that object.</p><p>You can set object permissions with profiles or permission sets. A user can have one profile and many permission sets.<ul><li>A user’s profile determines the objects they can access and the things they can do with any object record (such as create, read, edit, or delete).</li><li>Permission sets grant additional permissions and access settings to a user.</li></ul></p><p>Use profiles to grant the minimum permissions and settings that all users of a particular type need. Then use permission sets to grant more permissions as needed. The combination of profiles and permission sets gives you a great deal of flexibility in specifying object-level access.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Use Profiles to Restrict Access</b><p>Each user has a single profile that controls which data and features that user has access to. A profile is a collection of settings and permissions. Profile settings determine which data the user can see, and permissions determine what the user can do with that data.<ul><li>The settings in a user’s profile determine whether the user can see a particular app, tab, field, or record type.</li><li>The permissions in a user’s profile determine whether the user can create or edit records of a given type, run reports, and customize the app.</li></ul></p><p>Profiles usually match up with a user's job function (for example, system administrator, recruiter, or hiring manager), but you can have profiles for anything that makes sense for your Salesforce org. A profile can be assigned to many users, but a user can have only one profile at a time.</p>",
        "image": ""
      }
    ]
  },
  {
    "id": 1600,
    "date": "2020/11/26",
    "title": "Salesforce Dashboard",
    "itemList": [
      {
        "text": "<p>A dashboard is a visual display of key metrics and trends for records in your org. The relationship between a dashboard component and report is 1:1; for each dashboard component, there is a single source report. However, you can use the same report in multiple dashboard components on a single dashboard (for example, use the same report in both a bar chart and pie chart). You can display multiple dashboard components on a single dashboard page, creating a powerful visual display and a way to consume multiple reports that often have a common theme, like sales performance or customer support.</p>",
        "image": ""
      },
      {
        "text": "<p>Like reports, dashboards are stored in folders, which control who has access. If you have access to a folder, you can view its dashboards. However, to view the dashboard components, you need access to the underlying reports as well.</p>",
        "image": ""
      },
      {
        "text": "<p>Each dashboard has a running user, whose security settings determine which data to display in a dashboard. If the running user is a specific user, all dashboard viewers see data based on the security settings of that user—regardless of their own personal security settings. For this reason, you’ll want to choose the running user wisely, so as not to open up too much visibility. For example, set the sales manager as the running user for a leaderboard for her team. This allows her team members to view the leaderboard for their individual team, but not other teams.</p>",
        "image": ""
      },
      {
        "text": "<p>Dynamic dashboards are dashboards for which the running user is always the logged-in user. This way, each user sees the dashboard according to his or her own access level. If you’re concerned about too much access, dynamic dashboards might be the way to go.</p>",
        "image": ""
      }
    ]
  },
  {
    "id": 1700,
    "date": "2020/11/28",
    "title": "Salesforce Object",
    "itemList": [
      {
        "text": "<b>Relationships Among Objects</b><p><ul><li>Master-Detail (1:n) — A parent-child relationship in which the master object controls certain behaviors of the detail object.</li><li>Many-to-many — You can use master-detail relationships to model many-to-many relationships between any two objects</li><li>Lookup (1:n) — This type of relationship links two objects together, but has no effect on deletion or security. Unlike master-detail fields, lookup fields are not automatically required. When you define a lookup relationship, data from one object can appear as a custom related list on page layouts for the other object.</li></ul></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 1800,
    "date": "2020/11/28",
    "title": "Salesforce DML",
    "itemList": [
      {
        "text": "<p>Apex contains the built-in Database class, which provides methods that perform DML operations and mirror the DML statement counterparts.</p><p>Unlike DML statements, Database methods have an optional allOrNone parameter that allows you to specify whether the operation should partially succeed. When this parameter is set to false, if errors occur on a partial set of records, the successful records will be committed and errors will be returned for the failed records. Also, no exceptions are thrown with the partial success option.</p>",
        "image": ""
      },
      {
        "text": "<b>Should You Use DML Statements or Database Methods?</b><p><ul><li>Use DML statements if you want any error that occurs during bulk DML processing to be thrown as an Apex exception that immediately interrupts control flow (by using try. . .catch blocks). This behavior is similar to the way exceptions are handled in most database procedural languages.</li><li>Use Database class methods if you want to allow partial success of a bulk DML operation—if a record fails, the remainder of the DML operation can still succeed. Your application can then inspect the rejected records and possibly retry the operation. When using this form, you can write code that never throws DML exception errors. Instead, your code can use the appropriate results array to judge success or failure. Note that Database methods also include a syntax that supports thrown exceptions, similar to DML statements.</li></ul></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 1850,
    "date": "2020/12/11",
    "title": "SOQL",
    "itemList": [
      {
        "text": "<p>Instead of using the equal operator (=) for comparison, you can perform fuzzy matches by using the LIKE operator. For example, you can retrieve all accounts whose names start with SFDC by using this condition: WHERE Name LIKE 'SFDC%'. The % wildcard character matches any or no character. The _ character in contrast can be used to match just one character.</p>",
        "image": ""
      }
    ]
  },
  {
    "id": 1900,
    "date": "2020/11/27",
    "title": "Apex",
    "itemList": [
      {
        "text": "<p>Apex is a case-insensitive language.</p><p>Apex is tightly integrated with the database, you can access Salesforce records and their fields directly from Apex. Every record in Salesforce is natively represented as an sObject in Apex.</p>",
        "image": ""
      },
      {
        "text": "<p>Unlike specific sObjects types, generic sObjects can be created only through the newSObject() method. Also, the fields of a generic sObject can be accessed only through the put() and get() methods.</p><p>You can delete persisted records using the delete statement. Deleted records aren’t deleted permanently from Lightning Platform, but they’re placed in the Recycle Bin for 15 days from where they can be restored.</p>",
        "image": ""
      }
    ]
  },
  {
    "id": 1950,
    "date": "2020/12/12",
    "title": "Apex Triggers",
    "itemList": [
      {
        "text": "<p>To access the records that caused the trigger to fire, use context variables. For example, Trigger.New contains all the records that were inserted in insert or update triggers. Trigger.Old provides the old version of sObjects before they were updated in update triggers, or a list of deleted sObjects in delete triggers. Triggers can fire when one record is inserted, or when many records are inserted in bulk via the API or Apex. Therefore, context variables, such as Trigger.New, can contain only one record or multiple records. You can iterate over Trigger.New to get each individual sObject.</p>",
        "image": ""
      },
      {
        "text": "<p>The system saves the records that fired the before trigger after the trigger finishes execution. You can modify the records in the trigger without explicitly calling a DML insert or update operation. If you perform DML statements on those records, you get an error.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Trigger Context Variables</b></p>",
        "image": "../assets/image/server/otherProduct/triggerContextVariabe.PNG",
        "imageClass": "mx-auto d-block"
      },
      {
        "text": "<p><b>Triggers and Callouts</b><br>Apex allows you to make calls to and integrate your Apex code with external Web services. Apex calls to external Web services are referred to as callouts. For example, you can make a callout to a stock quote service to get the latest quotes. When making a callout from a trigger, the callout must be done asynchronously so that the trigger process doesn’t block you from working while waiting for the external service's response.The asynchronous callout is made in a background process, and the response is received when the external service returns it.</p><p>To make a callout from a trigger, call a class method that executes asynchronously. Such a method is called a future method and is annotated with @future(callout=true). This example class contains the future method that makes the callout.</p>",
        "image": ""
      }
    ]
  },
  {
    "id": 2000,
    "date": "2020/11/29",
    "title": "Apex Test",
    "itemList": [
      {
        "text": "<p>Salesforce records that are created in test methods aren’t committed to the database. They’re rolled back when the test finishes execution. This rollback behavior is handy for testing because you don’t have to clean up your test data after the test executes.</p><p>Whenever you modify your Apex code, rerun your tests to refresh code coverage results.</p><p>A known issue with the Developer Console prevents it from updating code coverage correctly when running a subset of tests. To update your code coverage results, use Test | Run All rather than Test | New Run.</p><p>By default, Apex tests don’t have access to pre-existing data in the org, except for access to setup and metadata objects, such as the User or Profile objects. Set up test data for your tests. Creating test data makes your tests more robust and prevents failures that are caused by missing or changed data in the org. You can create test data directly in your test method, or by using a utility test class as you’ll find out later.</p><p>Even though it is not a best practice to do so, there are times when a test method needs access to pre-existing data. To access org data, annotate the test method with @isTest(SeeAllData=true). The test method examples in this unit don’t access org data and therefore don’t use the SeeAllData parameter.</p>",
        "image": ""
      },
      {
        "text": "<p>The test method contains the Test.startTest() and Test.stopTest() method pair, which delimits a block of code that gets a fresh set of governor limits. In this test, test-data setup uses two DML statements before the test is performed. To test that Apex code runs within governor limits, isolate data setup’s limit usage from your test’s. To isolate the data setup process’s limit usage, enclose the test call within the Test.startTest() and Test.stopTest() block. Also use this test block when testing asynchronous Apex. For more information, see Using Limits, startTest, and stopTest.</p>",
        "image": ""
      }
    ]
  },
  {
    "id": 2100,
    "date": "2020/12/01",
    "title": "Salesforce Page Layout",
    "itemList": [
      {
        "text": "<p>There are two ways to customize a page in Lightning Experience. You can customize a page’s layout, or customize its contents. These are done with separate tools.</p><p>Lightning pages are a collection of Lightning components arranged in regions on the page. You can customize the structure of the page and the position of its components with the Lightning App Builder (learn more in the Lightning App Builder module right here on Trailhead).</p><p>You can customize a page’s contents, such as the fields and buttons that appear on the page, by using a different tool called the <b>page layout editor</b>. The page layout editor, also known as page layouts, helps you manage the content of pages in both our Classic UI and in Lightning Experience. The page layout editor is what we’ll be working with in this unit.</p>",
        "image": ""
      }
    ]
  },
  {
    "id": 2200,
    "date": "2020/12/03",
    "title": "Copado Branch Management",
    "itemList": [
      {
        "text": "<p>If there is no track of metadata changes in the different sandboxes, leverage Copado's Org Differences and Snapshot Differences features to compare the metadata differences between the production org and UAT/int/dev sandboxes.</p><p>Alternatively, the Git Snapshot feature lets you commit the metadata to all your environments to different branches of a Git repository. Once all the metadata in the different environments is committed to the branches, create pull requests between the branches to compare the differences.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Promotions</b><p>When the user story is promoted, Copado creates a promotion branch out of the destination branch. Then the feature branch(es) of the promoted user story(ies) get merged into the promotion branch, with the version of the files as in the feature branch(es), not the source org branch.</p><p>If you are promoting multiple user stories, feature branches get merged in ascending order based on the user story number. If there are overlapped metadata (e.g. 2 or more user stories contain the same Apex Class). Copado will try first a Git merge, but if there is a Git conflict, the feature branch version of the file will win over the promotion branch. This could lead to the last user story overwriting other user stories in the same promotion. This situation can be detected in advance with the overlap awareness feature. If Copado auto-resolves conflicts, you will receive an email with the file name(s) where the conflicts were auto-resolved. You can also review the merge in the promotion branch in the Git repository. You then need to sync the feature branches so that they have the same version of the overlapped file.</p><p>Exclude user stories created in UAT and integration from CBM, to avoid back porting them to the dev sandboxes. You can exclude them by checking the Exclude from CBM checkbox in the user story layout.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Destructive Changes</b><p>Destructive Changes is a Git operation that can be executed from the Git Snapshot and the User Story Commit pages and allows you to remove components both in Git and in your sandboxes. If you want to delete a component in Git and also in the destination sandboxes, you should use this functionality in a user story. If you just want to delete the component in Git,  you can use it in a Git snapshot.</p><p>When you select this operation, you can see the org credential and the metadata grid with the metadata components of the environment related to the org credential selected. If the component does not appear in the grid because it has already been deleted in the source org, then select a different org credential in the lookup field which corresponds to an org that still has the item to be deleted.</p>",
        "image": ""
      }
    ]
  },
  {
    "id": 2300,
    "date": "2020/12/04",
    "title": "Copado Org Differences",
    "itemList": [
      {
        "text": "<p>Org differences are calculated based on the last modified dates of the components in the source and destination orgs. To find metadata differences in content, use the Snapshot differences feature instead.</p>",
        "image": ""
      },
      {
        "text": "<p>When calculating org differences, a list of metadata is retrieved for the two orgs, and if the last modified date of a component is greater in the source environment, the component will be flagged as an update. If the last modified date is greater in the destination environment, no 'updated' flag will be shown.</p>",
        "image": ""
      },
      {
        "text": "<p>If a component experienced changes, but these were reverted, this component will appear as an update difference even if the content of the component is the same in both orgs. The last modified date can also be different if the component was deployed to the source org without any content/configuration changes.</p>",
        "image": ""
      }
    ]
  },
  {
    "id": 2400,
    "date": "2020/12/04",
    "title": "Copado Continuous Delivery",
    "itemList": [
      {
        "text": "<p>Copado Continuous Delivery makes use of Salesforce’s Change Data Capture (CDC) capability to enable automated and scheduled deployments. If you specify a scheduled or automated deployment behavior, please make sure you have enabled Change Data Capture in the Setup UI for two particular Copado objects: User Story and Promotion.</p>",
        "image": ""
      },
      {
        "text": "<p>It is possible to use Continuous Delivery to allow developers to deploy their work to different environments, even if they don’t have their own org credentials for that particular environment. To do this, simply set a default org credential for these environments. The default org credential will enable Continuous Delivery to deploy user stories to that environment, even if the developer doesn’t have their own access.</p>",
        "image": ""
      },
      {
        "text": "<p>Only someone with the Copado Admin license can deploy to production orgs. If you want to enable Continuous Delivery to automatically deploy certain kinds of user stories to production orgs, someone with a Copado Admin license will need to assign a default org credential to the production org.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Pipeline Page</b><br><ul><li>If the deployment has been successful, you will see a green check.</li><li>If the deployment has failed, a red cross will be displayed.</li><li>If the deployment is still in progress, you will see an in-progress icon.</li><li>If the deployment is waiting for a manual action to be completed, you will see a pause icon.</li><li>If there is a merge conflict you will see a yellow warning icon.</li></ul></p>",
        "image": ""
      },
      {
        "text": "<p>When using the promotion process, Copado combines the metadata (typically configuration) and Git metadata (typically code) to produce one single deployment record which will contain multiple steps.</p>",
        "image": ""
      }
    ]
  },
  {
    "id": 2500,
    "date": "2020/12/05",
    "title": "Copado Git Operations",
    "itemList": [
      {
        "text": "<p><b>Commit Files:</b> The Commit Files operation is available in the Git Snapshot and the User Story Commit pages. This operation allows you to commit specific metadata components of an org into a Git branch. You can use it to select new components that have not yet been committed in a user story or to commit an update in components that have been previously committed.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Recommit Files:</b> The Recommit Files operation is available in the User Story Commit page. This operation allows you to commit previously committed changes. When this operation is selected, all the components that have been previously selected and committed in the user story (User Story Selections) are automatically selected in the metadata grid, allowing you to speed up the process of updating your feature branch with the newest content in your org.</p><p>Additionally, if you have committed something wrong and want to get rid of the selection, this operation allows you to recreate the feature branch by checking the Re-create Feature Branch checkbox.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Destructive Changes:</b> The Destructive Changes operation is available in the Git Snapshot and the User Story Commit pages. This operation allows you to remove components both in Git and in your sandboxes.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Full Profiles & Permission Sets:</b> The Full Profiles & Permission Sets operation is available in the User Story Commit page. This operation allows you to commit full profiles and permission sets from one source org into Git. You should only use this operation for profiles and permission sets that are new and don't exist in the repository as the file will be overriding any other changes if a conflict is found.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Retrieve Only</b><br>With the Retrieve Only checkbox you will be able to pull OLS/FLS for profiles and permission sets, without having to commit/deploy the original fields or objects.<p>This feature gives you a more granular control over profiles and permission sets in regard to objects and fields permissions.</p></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 2600,
    "date": "2020/12/05",
    "title": "Simple and Parent Metadata Types",
    "itemList": [
      {
        "text": "<p>A simple metadata type is a metadata component that represents one file in Git (1:1), such as an ApexClass, an ApexTrigger, an ApexPage or an ApexComponent, for instance. The majority of metadata types fall within this category, so there is no need to list them all. If a metadata type is not included in the parent list or Special Cases section below, it is a simple metadata type.</p>",
        "image": ""
      },
      {
        "text": "<p>A parent metadata type is that where a metadata component represents one file in Git (1:1), but that component contains other nested components. Find below a table with all the parent metadata types and their corresponding nested components:</p>",
        "image": "../assets/image/server/otherProduct/parentMetadataType.PNG",
        "imageClass": "mx-auto d-block"
      },
      {
        "text": "<p><b>Profile</b><br><ul><li>When you commit a profile, only user permissions are included in the profile unless you select other components as well. To deploy the Field Level Security of a field, for example, you have to select the profile together with the field in the same selection, the same commit. The same applies to tab visibility, where you have to include the CustomTab to get the visibility of that tab.</li><li>The most typical metadata component updates in a profile are CustomObject, CustomField, CustomTab, Layout, RecordType, ApexClass, ApexPage, CustomApplication, CustomPermission, and HomePageLayout.</li><li>Copado will merge the retrieved permissions into the profile in Git, unless you commit a full profile, in which case the file in the feature branch will overwrite the file in the promotion branch.</li><li>When committing files, if you select the Retrieve Only checkbox for custom objects, custom fields and page layouts, you will be able to pull OLS/FLS/Layout Assignment without having to commit/deploy the original field/object/layout files. Check out the article User Stories - Git Metadata to get more information.</li></ul></p>",
        "image": ""
      },
      {
        "text": "<p><b>Permission Set</b><br><ul><li>Permission sets behave in the same way as profiles, only permissions for other selected metadata components will be included.</li><li>Copado will merge the retrieved permissions into the permission set in Git, unless you commit a full permission set, in which case the permission set file in the feature branch will overwrite the file in the promotion branch.</li><li>You can select the Retrieve Only checkbox for custom objects and custom fields to pull OLS/FLS without having to commit/deploy the original field/object files. Check out the article User Stories - Git Metadata to get more information.</li></ul></p>",
        "image": ""
      },
      {
        "text": "<p><b>Custom Object</b><br>In contrast to the standard behavior of the metadata API, when you commit a custom object only the attributes are committed, unless you select other components as well. This applies to new and existing custom objects.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Custom Object Translation</b><br>Custom object translations are handled as nested components. Let’s say you want to commit the custom object translation of a field and you just commit that field together with the translation. By doing this, Copado will only update the translation of that field in the destination, and no other field translations will be committed or merged.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Flow</b><br>Deploying a Process Builder Flow in Copado is a seamless process.You can deploy a flow that is active in the source org, and the flow will be activated automatically in the destination org. You can commit an active flow to your repository and deploy from the commit. Check out the article Process Builder Flows to get more information.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Flow Definition</b><br>A FlowDefinition can only be deployed if the flow already exists in the destination. In order to make a deployment not repeatable, the recommendation is not to include this in the source control system (Git) by adding it to your .gitignore file.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Important Considerations</b><br><ul><li>A feature branch will only be created if the metadata retrieved is different from the repository version.</li><li>If a feature branch already exists, and the metadata retrieved is different, the new version will be added to the feature branch. If there are no changes, Git does not produce a commit ID and Copado will show the status No changes.</li><li>If the feature branch does not exist and the metadata selected to be committed has no changes, the feature branch will not be pushed to your repository, and as a consequence, the branch will not exist in your repository.</li><li>As part of the Update User Story selections action, Copado upserts some other auxiliary records and fields such as Git commits and metadata types in selections.</li></ul></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 2700,
    "date": "2020/12/05",
    "title": "Copado Validating Commits",
    "itemList": [
      {
        "text": "<p>From the Commit Changes page, Copado allows you to perform a validation deployment before actually committing the changes into Git.</p><p>The process in order to have this functionality working:</p><p><ul><li>Include in the commit message the following pattern: @validatecommit.</li><li>This command will create the validation deployment without committing any changes into the repository.</li><li>If the deployment is successful, the commit will be executed.</li><li>The validation of a commit relays on a validation deployment in the destination org with the selected metadata components selected in the commit changes action.</li></ul></p>",
        "image": ""
      },
      {
        "text": "<p>Recommit Files: The Recommit Files operation is available in the User Story Commit page. This operation allows you to commit previously committed changes. When this operation is selected, all the components that have been previously selected and committed in the user story (User Story Selections) are automatically selected in the metadata grid, allowing you to speed up the process of updating your feature branch with the newest content in your org.</p><p>Additionally, if you have committed something wrong and want to get rid of the selection, this operation allows you to recreate the feature branch by checking the Re-create Feature Branch checkbox.</p>",
        "image": ""
      },
      {
        "text": "<p>Destructive Changes: The Destructive Changes operation is available in the Git Snapshot and the User Story Commit pages. This operation allows you to remove components both in Git and in your sandboxes.</p>",
        "image": ""
      },
      {
        "text": "<p>Full Profiles & Permission Sets: The Full Profiles & Permission Sets operation is available in the User Story Commit page. This operation allows you to commit full profiles and permission sets from one source org into Git. You should only use this operation for profiles and permission sets that are new and don't exist in the repository as the file will be overriding any other changes if a conflict is found.</p>",
        "image": ""
      }
    ]
  },
  {
    "id": 2800,
    "date": "2020/12/06",
    "title": "Manual Task deployment",
    "itemList": [
      {
        "text": "<p>To create a Manual Task deployment, follow these steps:</p><p><ul><li>Navigate to the Deployments tab to create a new Advanced (multi-step) deployment.</li><li>Give your deployment a name and select a source and a destination organization.</li><li>Add a new step and select Manual Task as step type.</li><li>Optionally, add a task owner and select how you want this user to be notified, via chatter, via email or both.</li><li>When Copado reaches a Manual Task deployment step, the deployment process is stopped until the manual task is completed. The task owner will receive a notification for the task to be executed. Once the task is set to Complete, the deployment process will automatically resume</li></ul></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 2900,
    "date": "2020/12/07",
    "title": "Copado Dynamic Variables",
    "itemList": [
      {
        "text": "<p>A variable is a container that stores a piece of data. Variables can be used in a variety of cases, for instance, if you are deploying users to different environments and you want to make sure the email address is the correct one in each environment, or if you are working with Salesforce Flow deployment steps.</p><p>When working with a Salesforce Flow deployment step, Copado provides the following predefined dynamic variables:</p>",
        "image": "../assets/image/server/otherProduct/dynamicVariables.PNG",
        "imageClass": "mx-auto d-block"
      }
    ]
  },
  {
    "id": 3000,
    "date": "2020/12/07",
    "title": "Copado Features vs. Epics",
    "itemList": [
      {
        "text": "<p>Epics typically define the details of an MVP (Minimum Viable Product) and have a limited scope which could span across multiple sprints but are confined to a single release. Epics have no defined start or end date but do have defined scope. Epics can be written as larger user stories and should include the Who, What , and Why. Features are much smaller than epics and can live forever as features may evolve with the product’s lifecycle. Features capture the business value and impact. Features should be developed within a single release, but can live beyond releases and sprints.</p>",
        "image": ""
      }
    ]
  }
]