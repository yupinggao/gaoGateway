[
  {
    "id": 100,
    "date": "2019/12/22",
    "title": "K8S Concepts",
    "itemList": [
      {
        "text": "<p>To work with Kubernetes, you use Kubernetes API objects to describe your cluster’s desired state: what applications or other workloads you want to run, what container images they use, the number of replicas, what network and disk resources you want to make available, and more. You set your desired state by creating objects using the Kubernetes API, typically via the command-line interface, kubectl. You can also use the Kubernetes API directly to interact with the cluster and set or modify your desired state.<p>Once you’ve set your desired state, the Kubernetes Control Plane makes the cluster’s current state match the desired state via the Pod Lifecycle Event Generator (PLEG). To do so, Kubernetes performs a variety of tasks automatically–such as starting or restarting containers, scaling the number of replicas of a given application, and more. The Kubernetes Control Plane consists of a collection of processes running on your cluster:</p></p>",
        "image": ""
      },
      {
        "text": "<p><b>Kubernetes Master runs three processes: </b><ul><li><b>kube-apiserver</b> - The Kubernetes API server validates and configures data for the api objects which include pods, services, replicationcontrollers, and others. The API Server services REST operations and provides the frontend to the cluster’s shared state through which all other components interact.</li><li><b>kube-controller-manager</b> - The Kubernetes controller manager is a daemon that embeds the core control loops shipped with Kubernetes. Examples of controllers that ship with Kubernetes today are the replication controller, endpoints controller, namespace controller, and serviceaccounts controller.</li><li><b>kube-scheduler</b> - The Kubernetes scheduler is a policy-rich, topology-aware, workload-specific function that significantly impacts availability, performance, and capacity. The scheduler needs to take into account individual and collective resource requirements, quality of service requirements, hardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference, deadlines, and so on. Workload-specific requirements will be exposed through the API as necessary.</li></ul></p>",
        "image": ""
      },
      {
        "text": "<p><b>non-master node in your cluster runs two processes: </b><ul><li><b>kubelet</b> - The kubelet is the primary “node agent” that runs on each node. It can register the node with the apiserver using one of: the hostname; a flag to override the hostname; or specific logic for a cloud provider.</li><li>kube proxy</li></ul></p>",
        "image": ""
      },
      {
        "text": "<p><b>Kubernetes Objects: </b><ul><li>Pod</li><li>Service</li><li>Volume</li><li>Namespace</li><li>Deployment</li><li><b>DaemonSet</b> - A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created.</li><li>StatefulSet</li><li>ReplicaSet</li><li>Job</li><li>NetworkPolicy</li><li><b>Addons:</b> CoreDNS, Dashboard etc.</li></ul></p>",
        "image": ""
      },
      {
        "text": "<p><b>Architecture of a Kubernetes cluster without the cloud controller manager:</b></p>",
        "image": "../assets/image/itCloud/kubernetes/archDiagram1.png",
        "imageClass": "mx-auto d-block"
      },
      {
        "text": "<p><b>Architecture of a Kubernetes cluster with the cloud controller manager:</b></p>",
        "image": "../assets/image/itCloud/kubernetes/archDiagram2.png",
        "imageClass": "mx-auto d-block"
      }
    ]
  },
  {
    "id": 200,
    "date": "2019/12/27",
    "title": "Namespaces",
    "itemList": [
      {
        "text": "<p>Namespaces are intended for use in environments with many users spread across multiple teams, or projects. For clusters with a few to tens of users, you should not need to create or think about namespaces at all. Start using namespaces when you need the features they provide.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Kubernetes starts with three initial namespaces:</b><ul><li><b>default</b> The default namespace for objects with no other namespace</li><li><b>kube-system</b> The namespace for objects created by the Kubernetes system</li><li><b>kube-public</b> This namespace is created automatically and is readable by all users (including those not authenticated). This namespace is mostly reserved for cluster usage, in case that some resources should be visible and readable publicly throughout the whole cluster. The public aspect of this namespace is only a convention, not a requirement</li></ul></p>",
        "image": ""
      },
      {
        "text": "<p>You can permanently save the namespace for all subsequent kubectl commands in that context:<br><code>kubectl config set-context --current --namespace=<i>insert-namespace-name-here</i><code><br><code>kubectl config view --minify | grep namespace:</code> - validate it</p>",
        "image": ""
      },
      {
        "text": "<p>Not All Objects are in a Namespace. To see which Kubernetes resources are and aren’t in a namespace: <br><code>kubectl api-resources --namespaced=true</code><br><code>kubectl api-resources --namespaced=false</code></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 300,
    "date": "2019/12/26",
    "title": "Object Management",
    "itemList": [
      {
        "text": "<p><b>Management techniques</b> - Warning: A Kubernetes object should be managed using only one technique. Mixing and matching techniques for the same object results in undefined behavior.</p>",
        "image": "../assets/image/itCloud/kubernetes/objectMan.png",
        "imageClass": "mx-auto d-block"
      },
      {
        "text": "<p><b>Imperative commands</b><ul><li><code>kubectl run nginx --image nginx</code> - run an instance of the nginx container by creating a Deployment object</li><li><code>kubectl create deployment nginx --image nginx</code> - do the same thing using a different syntax</li></ul></p>",
        "image": ""
      },
      {
        "text": "<p><b>Imperative object configuration</b><ul><li><code>kubectl create -f nginx.yaml</code> - create the objects defined in a configuration file</li><li><code>kubectl delete -f nginx.yaml -f redis.yaml</code> - delete the objects defined in two configuration files</li><li><code>kubectl replace -f nginx.yaml</code> - update the objects defined in a configuration file by overwriting the live configuration</li></ul></p>",
        "image": ""
      },
      {
        "text": "<p><b>Declarative object configuration</b><ul><li><code>kubectl diff -f configs/</code><br><code>kubectl apply -f configs/</code> - process all object configuration files in the configs directory, and create or patch the live objects. You can first diff to see what changes are going to be made, and then apply</li><li><code>kubectl diff -R -f configs/</code><br><code>kubectl apply -R -f configs/</code> - recursively process directorie</li></ul></p>",
        "image": ""
      },
      {
        "text": "<p><b>Migrating from imperative command management to declarative object configuration</b><ol><li>Export the live object to a local configuration file:<br><code>kubectl get <i>kind</i>/<i>name</i> -o yaml > <i>kind_name</i>.yaml</code></li><li>Manually remove the status field from the configuration file.</li><li>Set the kubectl.kubernetes.io/last-applied-configuration annotation on the object:<br><code>kubectl replace --save-config -f <i>kind_name</i>.yaml</code></li><li>Change processes to use kubectl apply for managing the object exclusively.</li></ol></p>",
        "image": ""
      },
      {
        "text": "<p><b>Migrating from imperative object configuration to declarative object configuration</b><ol><li>Set the kubectl.kubernetes.io/last-applied-configuration annotation on the object:<br><code>kubectl replace --save-config -f <i>kind_name</i>.yaml</code></li><li>Change processes to use kubectl apply for managing the object exclusively.</li></ol></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 400,
    "date": "2020/01/11",
    "title": "Configuration Best Practices",
    "itemList": [
      {
        "text": "<p><b>General Configuration Tips</b><ul><li>When defining configurations, specify the latest stable API version.</li><li>Configuration files should be stored in version control before being pushed to the cluster. This allows you to quickly roll back a configuration change if necessary. It also aids cluster re-creation and restoration.</li><li>Write your configuration files using YAML rather than JSON. Though these formats can be used interchangeably in almost all scenarios, YAML tends to be more user-friendly.</li><li>Group related objects into a single file whenever it makes sense. One file is often easier to manage than several. See the guestbook-all-in-one.yaml file as an example of this syntax.</li><li>Note also that many kubectl commands can be called on a directory. For example, you can call kubectl apply on a directory of config files.</li><li>Don’t specify default values unnecessarily: simple, minimal configuration will make errors less likely.</li><li>Put object descriptions in annotations, to allow better introspection.</li><ul></p>",
        "image": ""
      },
      {
        "text": "<p><b>'Naked' Pods vs ReplicaSets, Deployments, and Jobs</b></p><p>Don’t use naked Pods (that is, Pods not bound to a ReplicaSet or Deployment) if you can avoid it. Naked Pods will not be rescheduled in the event of a node failure.<br>A Deployment, which both creates a ReplicaSet to ensure that the desired number of Pods is always available, and specifies a strategy to replace Pods (such as RollingUpdate), is almost always preferable to creating Pods directly, except for some explicit restartPolicy: Never scenarios. A Job may also be appropriate.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Services</b></p><p><ul><li>Create a Service before its corresponding backend workloads (Deployments or ReplicaSets), and before any workloads that need to access it. When Kubernetes starts a container, it provides environment variables pointing to all the Services which were running when the container was started. For example, if a Service named foo exists, all containers will get the following variables in their initial environment:<br>&nbsp;&nbsp;&nbsp;&nbsp;FOO_SERVICE_HOST=<i>the host the Service is running on</i><br>&nbsp;&nbsp;&nbsp;&nbsp;FOO_SERVICE_PORT=<i>the port the Service is running on</></li><li>An optional (though strongly recommended) cluster add-on is a DNS server. The DNS server watches the Kubernetes API for new Services and creates a set of DNS records for each. If DNS has been enabled throughout the cluster then all Pods should be able to do name resolution of Services automatically.</li><li>Don’t specify a hostPort for a Pod unless it is absolutely necessary. When you bind a Pod to a hostPort, it limits the number of places the Pod can be scheduled, because each <hostIP, hostPort, protocol> combination must be unique. If you don’t specify the hostIP and protocol explicitly, Kubernetes will use 0.0.0.0 as the default hostIP and TCP as the default protocol.</li><li>Avoid using hostNetwork, for the same reasons as hostPort.</li><li>Use headless Services (which have a ClusterIP of None) for easy service discovery when you don’t need kube-proxy load balancing.</li></ul></p>",
        "image": ""
      },
      {
        "text": "<p><b>Using Labels</b></p><p><ul><li>Define and use labels that identify semantic attributes of your application or Deployment, such as { app: myapp, tier: frontend, phase: test, deployment: v3 }. You can use these labels to select the appropriate Pods for other resources; for example, a Service that selects all tier: frontend Pods, or all phase: test components of app: myapp.</li><li>You can manipulate labels for debugging. Because Kubernetes controllers (such as ReplicaSet) and Services match to Pods using selector labels, removing the relevant labels from a Pod will stop it from being considered by a controller or from being served traffic by a Service. If you remove the labels of an existing Pod, its controller will create a new Pod to take its place. This is a useful way to debug a previously “live” Pod in a “quarantine” environment. To interactively remove or add labels, use kubectl label.</li></ul></p>",
        "image": ""
      },
      {
        "text": "<p><b>Node isolation/restriction</b></p><p>Adding labels to Node objects allows targeting pods to specific nodes or groups of nodes. This can be used to ensure specific pods only run on nodes with certain isolation, security, or regulatory properties. When using labels for this purpose, choosing label keys that cannot be modified by the kubelet process on the node is strongly recommended. This prevents a compromised node from using its kubelet credential to set those labels on its own Node object, and influencing the scheduler to schedule workloads to the compromised node.</p><p>The NodeRestriction admission plugin prevents kubelets from setting or modifying labels with a node-restriction.kubernetes.io/ prefix. To make use of that label prefix for node isolation:</p><p><ol><li>Ensure you are using the Node authorizer and have enabled the NodeRestriction admission plugin.</li><li>Add labels under the node-restriction.kubernetes.io/ prefix to your Node objects, and use those labels in your node selectors. For example, example.com.node-restriction.kubernetes.io/fips=true or example.com.node-restriction.kubernetes.io/pci-dss=true.</li></ol></p>",
        "image": ""
      },
      {
        "text": "<p><b>Container Images</b></p><p><ul><li>You should avoid using the :latest tag when deploying containers in production as it is harder to track which version of the image is running and more difficult to roll back properly.</li><li>To make sure the container always uses the same version of the image, you can specify its digest, for example sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2. The digest uniquely identifies a specific version of the image, so it is never updated by Kubernetes unless you change the digest value.</li></ul></p>",
        "image": ""
      },
      {
        "text": "<p><b>Using kubectl</b></p><p><ul><li>Use kubectl apply -f <directory>. This looks for Kubernetes configuration in all .yaml, .yml, and .json files in <i>directory</i> and passes it to apply.</li><li>Use label selectors for get and delete operations instead of specific object names. See the sections on label selectors and using labels effectively.</li><li>Use kubectl run and kubectl expose to quickly create single-container Deployments and Services. See Use a Service to Access an Application in a Cluster for an example.</li></ul></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 500,
    "date": "2020/01/14",
    "title": "Node affinity, Taints and Tolerations",
    "itemList": [
      {
        "text": "<p>Node affinity, described here, is a property of pods that attracts them to a set of nodes (either as a preference or a hard requirement). Taints are the opposite – they allow a node to repel a set of pods.</p>",
        "image": ""
      },
      {
        "text": "<p>Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node; this marks that the node should not accept any pods that do not tolerate the taints. Tolerations are applied to pods, and allow (but do not require) the pods to schedule onto nodes with matching taints.</p>",
        "image": ""
      }
    ]
  },
  {
    "id": 600,
    "date": "2019/12/23",
    "title": "Deployment",
    "itemList": [
      {
        "text": "<p><b>Proportional Scaling</b></p><p>RollingUpdate Deployments support running multiple versions of an application at the same time. When you or an autoscaler scales a RollingUpdate Deployment that is in the middle of a rollout (either in progress or paused), the Deployment controller balances the additional replicas in the existing active ReplicaSets (ReplicaSets with Pods) in order to mitigate risk. This is called proportional scaling.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Deployment status</b></p><p><ul><li>Progressing Deployment</li><li>Complete Deployment</li><li>Failed Deployment</li></ul></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 700,
    "date": "2019/12/28",
    "title": "Pod",
    "itemList": [
      {
        "text": "<p><b>Pod phase</b></p>",
        "image": "../assets/image/itCloud/kubernetes/podPhase.png",
        "imageClass": "mx-auto d-block"
      },
      {
        "text": "<p><b>Container probes</b></p><p>A Probe is a diagnostic performed periodically by the kubelet on a Container. To perform a diagnostic, the kubelet calls a Handler implemented by the Container. There are three types of handlers:<ol><li><b>ExecAction:</b> Executes a specified command inside the Container. The diagnostic is considered successful if the command exits with a status code of 0.</li><li><b>TCPSocketAction:</b> Performs a TCP check against the Container’s IP address on a specified port. The diagnostic is considered successful if the port is open.</li><li><b>HTTPGetAction:</b> Performs an HTTP Get request against the Container’s IP address on a specified port and path. The diagnostic is considered successful if the response has a status code greater than or equal to 200 and less than 400.</li></ol></p>",
        "image": ""
      },
      {
        "text": "<p><b>The kubelet can optionally perform and react to three kinds of probes on running Containers:</b></p><p><ol><li><b>livenessProbe: </b>Indicates whether the Container is running. If the liveness probe fails, the kubelet kills the Container, and the Container is subjected to its restart policy. If a Container does not provide a liveness probe, the default state is Success.</li><li><b>readinessProbe:</b> Indicates whether the Container is ready to service requests. If the readiness probe fails, the endpoints controller removes the Pod’s IP address from the endpoints of all Services that match the Pod. The default state of readiness before the initial delay is Failure. If a Container does not provide a readiness probe, the default state is Success.</li><li><b>startupProbe:</b> Indicates whether the application within the Container is started. All other probes are disabled if a startup probe is provided, until it succeeds. If the startup probe fails, the kubelet kills the Container, and the Container is subjected to its restart policy. If a Container does not provide a startup probe, the default state is Success.</li></ol></p>",
        "image": ""
      },
      {
        "text": "<p><b>Container States:</b></p><p><ul><li>Waiting</li><li>Running</li><li>Terminated</li></ul></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 800,
    "date": "2019/12/28",
    "title": "Container",
    "itemList": [
      {
        "text": "<p><b>Container Lifecycle Hooks</b></p><p>There are two hooks that are exposed to Containers:</p>",
        "image": ""
      },
      {
        "text": "<p><b>PostStart</b></p><p>This hook executes immediately after a container is created. However, there is no guarantee that the hook will execute before the container ENTRYPOINT. No parameters are passed to the handler.</p>",
        "image": ""
      },
      {
        "text": "<p><b>PreStop</b></p><p>This hook is called immediately before a container is terminated due to an API request or management event such as liveness probe failure, preemption, resource contention and others. A call to the preStop hook fails if the container is already in terminated or completed state. It is blocking, meaning it is synchronous, so it must complete before the call to delete the container can be sent. No parameters are passed to the handler.</p>",
        "image": ""
      }
    ]
  },
  {
    "id": 1000,
    "date": "2020/01/12",
    "title": "Volume Basics",
    "itemList": [
      {
        "text": "<p><b>emptyDir</b></p><p>emptyDir are volumes that get created empty when a Pod is created.</p><p>While a Pod is running its emptyDir exists. If a container in a Pod crashes the emptyDir content is unaffected. Deleting a Pod deletes all its emptyDirs.</p><p>There are several ways a Pod can be deleted. Accidental and deliberate. All result in immediate emptyDir deletion. emptyDir are meant for temporary working disk space.</p><p></p>",
        "image": ""
      },
      {
        "text": "<p><b>PersistentVolume</b></p><p>If you need persistent volumes you use: PersistentVolume and PersistentVolumeClaim</p><p>When the PersistentVolumeClaim is deleted, the PersistentVolume still exists and the volume is considered 'released'. But it is not yet available for another claim because the previous claimant's data remains on the volume.</p><p></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 1010,
    "date": "2020/01/03",
    "title": "Ingress in K8S",
    "itemList": [
      {
        "text": "<p>An API object that manages external access to the services in a cluster, typically HTTP. It can provide load balancing, SSL termination and name-based virtual hosting.</p><p>Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource. An Ingress controller is responsible for fulfilling the Ingress, usually with a load balancer, though it may also configure your edge router or additional frontends to help handle the traffic.</p>",
        "image": ""
      },
      {
        "text": "<p>Unlike other types of controllers which run as part of the kube-controller-manager binary, Ingress controllers are not started automatically with a cluster. You may need to deploy an Ingress controller such as ingress-nginx. Ideally, all Ingress controllers should fit the reference specification. In reality, the various Ingress controllers operate slightly differently.</p>",
        "image": ""
      }
    ]
  },
  {
    "id": 1020,
    "date": "2019/12/13",
    "title": "kubectl",
    "itemList": [
      {
        "text": "<p><code>kubectl -n kube-system logs -l=component=kube-apiserver</code> - to view the system log</p>",
        "image": ""
      },    
      {
        "text": "<p><code>kubectl annotate pods my-nginx-v4-9gw19 description='my frontend running nginx'</code></p>",
        "image": ""
      },      
      {
        "text": "<p><code>kubectl api-resources --namespaced=true</code> - see which Kubernetes resources are in a namespace</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl api-resources -o wide</code> - get API resources supported by your Kubernetes cluster</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl apply -f captureorder-deployment.yaml</code> - apply the instructions in the file</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta8/aio/deploy/recommended.yaml</code> - deploy the dashboard</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl apply --validate -f mypod.yaml</code> - create the pod and validate the manifest file</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl apply --validate=false -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.11/deploy/manifests/00-crds.yaml</code> - apply the instructions in the file</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl attach POD -c CONTAINER</code> - attach to a process that is already running inside an existing container</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl autoscale deployment.v1.apps/nginx-deployment --min=10 --max=15 --cpu-percent=80</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl cluster-info dump</code> - get detailed information about the overall health of your cluster</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl config current-context</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl config set-context --current --namespace=<i>namespace-name</i></code> - permanently save the namespace for all subsequent kubectl commands in that context</p>",
        "image": ""
      },      
      {
        "text": "<p><code>kubectl config unset users.foo</code> - delete user foo</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl config use-context <i>context_name</i></code> - to operate in the <i>context_name</i> namespace</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl config view</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl config view --minify</code> - to see only the configuration information associated with the current context</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl convert -f pod.yaml --output-version v1</code> - use kubectl convert command to convert config files between different API versions</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl cordon $NODENAME</code> - mark a Node unschedulable</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl create deployment hello-web --image=gcr.io/${PROJECT_ID}/hello-app:v1</code> - deploy your application image</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl create namespace ingress</code> - create a namespace for the ingress</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl expose deployment hello-node --type=LoadBalancer --port=8080</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl expose rc example --port=8765 --target-port=9376 --name=example-service --type=LoadBalancer</code> -  creates a new service using the same selectors as the referenced resource (in the case of the example above, a replication controller named example)</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl create secret generic mongodb --from-literal=mongoHost='orders-mongo-mongodb.default.svc.cluster.local' --from-literal=mongoUser='orders-user' --from-literal=mongoPassword='orders-password'</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl create secret docker-registry myregistrykey --docker-server=DUMMY_SERVER --docker-username=DUMMY_USERNAME --docker-password=DUMMY_DOCKER_PASSWORD --docker-email=DUMMY_DOCKER_EMAIL</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl delete cronjob <i>job-name</i></code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl delete service hello-web</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl describe deployments</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl describe certificate frontend</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl describe pod <pod name></code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl diff -f <file name></code> - print the object that will be created</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl drain <i>NODENAME</i></code> - to gracefully terminate all pods on the node while marking the node as unschedulable/p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl exec my-nginx-3800858182-jr4a2 -- printenv | grep SERVICE</code> - inspect the environment of your running nginx Pods</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl expose deployment hello-web --type=LoadBalancer --port 80 --target-port 8080</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get apiservices</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get componentstatuses</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get csr</code> - view the status of certificate signing requests</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get endpoints <i>service-name</i></code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get ep <i>endpoint_name</i></code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get events</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get events --all-namespaces</code> - to see events from all namespaces</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get namespaces --show-labels</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get nodes</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get pods --all-namespaces -o go-template --template='{{range .items}}{{range .spec.containers}}{{.image}} {{end}}{{end}}'</code> - to use go-templates for formatting the output.</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get pods --all-namespaces -o jsonpath='{..image}' | tr -s '[[:space:]]' '\n' | sort | uniq -c</code> - to list all Container images in all namespaces. Use tr to replace spaces with newlines. Use uniq to aggregate image counts.</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get pods --all-namespaces -o jsonpath='{.items[*].spec.containers[*].image}'</code> - to use the absolute path to the image field within the Pod.</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get pods -l app=captureorder -w</code> - watch the pod named 'captureorder'</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get pods -l run=my-nginx -o yaml | grep podIP</code> - check your pods’ IPs</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get pods --selector=job-name=pi --output=jsonpath='{.items[*].metadata.name}'</code> - list all the Pods that belong to a Job in a machine readable form</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get pods --show-labels</code> - see the labels automatically generated for each Pod</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get pvc</code> - find Persistent Volume Claims</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get rs</code> - see the ReplicaSet (rs)</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get secret --namespace default grafana -o jsonpath='{.data.admin-password}' | base64 --decode ; echo</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get secrets --all-namespaces -o json</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get secrets --all-namespaces -o json | kubectl replace -f -</code> - to force all secrets to be decrypted</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get service captureorder -o jsonpath='{.status.loadBalancer.ingress[*].ip}' -w</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get services</code></p>",
        "image": ""
      }, 
      {
        "text": "<p><code>kubectl get services kube-dns --namespace=kube-system</code> - check if DNS cluster addon Service is enabled</p>",
        "image": ""
      }, 
      {
        "text": "<p><code>kubectl get services  --all-namespaces --field-selector metadata.namespace!=default</code> - selects all Kubernetes Services that aren’t in the default namespace</p>",
        "image": ""
      }, 
      {
        "text": "<p><code>kubectl get services/kubernetes-bootcamp -o go-template='{{(index .spec.ports 0).nodePort}}'</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get svc  -n ingress    ingress-nginx-ingress-controller -o jsonpath='{.status.loadBalancer.ingress[*].ip}'</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl label pods -l app=nginx tier=fe</code> - label all your nginx pods as frontend tier</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl logs <i>pod name</i></code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl patch deployment.v1.apps/nginx-deployment -p '{'spec':{'progressDeadlineSeconds':600}}'</code> - set the spec with progressDeadlineSeconds to make the controller report lack of progress for a Deployment after 10 minutes</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl patch pod <pod> -p '{'metadata':{'finalizers':null}}'</code> - remove the pod from the cluster if other delete operation has failed</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl patch pv <your-pv-name> -p '{'spec':{'persistentVolumeReclaimPolicy':'Retain'}}'</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl port-forward svc/promitor-agent-scraper 8080:80</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl replace -f <i>YAML file</i></code> - delete and recreate a resource</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl rollout history deployment.v1.apps/nginx-deployment</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl rollout history deployment.v1.apps/nginx-deployment --revision=2</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl rollout pause deployment.v1.apps/nginx-deployment</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl rollout resume deployment.v1.apps/nginx-deployment</code> - You cannot rollback a paused Deployment until you resume it</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl rollout status deployment.apps/nginx-deployment</code> - view the rollout status for deployment.apps/nginx-deployment</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl rollout undo deployment.v1.apps/nginx-deployment</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl rollout undo deployment.v1.apps/nginx-deployment --to-revision=2</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl run -it --rm --image=mysql:5.6 --restart=Never mysql-client -- mysql -h mysql -ppassword</code> - creates a new Pod in the cluster running a MySQL client and connects it to the server through the Service. If it connects, you know your stateful MySQL database is up and running</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl run busybox --rm -ti --image=busybox -- /bin/sh</code> - run a job</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl scale deployment hello-web --replicas=3</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1 --record</code> - update the nginx Pods to use the nginx:1.16.1</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl uncordon <i>NODENAME</i></code> - to make the node schedulable again</p>",
        "image": ""
      }
    ]
  },
  {
    "id": 1030,
    "date": "2020/04/04",
    "title": "kubernetes CLI",
    "itemList": [
      {
        "text": "<p><code>for p in $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name); do kubectl logs --namespace=kube-system $p; done</code> - check for Errors in the DNS pod</p>",
        "image": ""
      }
    ]
  },
  {
    "id": 1040,
    "date": "2019/12/18",
    "title": "Minikube",
    "itemList": [
      {
        "text": "<p><b>Update Minikube</b></p>",
        "image": ""
      },
      {
        "text": "<p><ol><li><code>sudo minikube delete</code></li><li><code>sudo curl -Lo minikube https://storage.googleapis.com/minikube/releases/v1.3.1/minikube-linux-amd64 && sudo chmod +x minikube && sudo cp minikube /usr/local/bin/ && sudo rm minikube</code></li><li><code>minikube version</code></li><li><code>sudo minikube start --vm-driver=none</code></li></ol></p>",
        "image": "../assets/image/itCloud/kubernetes/minikube.png",
        "imageClass": "mx-auto d-block"
      },
      {
        "text": "<p><ol><li><code>minikube stop</code></li><li><code>minikube config set memory 3072</code></li><li><code>minikube delete -p minikube</code></li><li><code>minikube start</code></li></ol></p>",
        "image": ""
      },
      {
        "text": "<p><code>minikube dashboard</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>minikube ip</code> - get the LB service IP</p>",
        "image": ""
      },
      {
        "text": "<p><b>addons</b><br><ul><li><code>minikube addons list</code></li><li>minikube addons enable ingress</li></ul></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 2000,
    "date": "2020/01/12",
    "title": "Terminology",
    "itemList": [
      {
        "text": "<p><b>Millicores</b> - Kubernetes has a new metric called Millicores that is used to measure CPU usage. It is a CPU core split into 1000 units (milli = 1000). If you have 4 cores, then the CPU capacity of the node is 4000m.</p>",
        "image": ""
      }
    ]
  },
  {
    "id": 2050,
    "date": "2020/05/05",
    "title": "Logs",
    "itemList": [
      {
        "text": "<p><b>System component logs</b><p>There are two types of system components: those that run in a container and those that do not run in a container. For example:<ul><li>The Kubernetes scheduler and kube-proxy run in a container.</li><li>The kubelet and container runtime, for example Docker, do not run in containers.</li></ul></p><p>On machines with systemd, the kubelet and container runtime write to journald. If systemd is not present, they write to .log files in the /var/log directory. System components inside containers always write to the /var/log directory, bypassing the default logging mechanism. They use the klog logging library. You can find the conventions for logging severity for those components in the development docs on logging.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Cluster-level logging architectures</b><p>While Kubernetes does not provide a native solution for cluster-level logging, there are several common approaches you can consider. Here are some options:</p>",
        "image": ""
      },
      {
        "text": "<p>Use a node-level logging agent that runs on every node</p>",
        "image": "../assets/image/itCloud/kubernetes/clusterLog1.PNG",
        "imageClass": "mx-auto d-block"
      },
      {
        "text": "<p>Include a dedicated sidecar container for logging in an application pod</p>",
        "image": "../assets/image/itCloud/kubernetes/clusterLog2.PNG",
        "imageClass": "mx-auto d-block"
      },
      {
        "text": "<p>Push logs directly to a backend from within an application</p>",
        "image": "../assets/image/itCloud/kubernetes/clusterLog3.PNG",
        "imageClass": "mx-auto d-block"
      }
    ]
  },
  {
    "id": 2100,
    "date": "2020/02/27",
    "title": "kubeadmin",
    "itemList": [
      {
        "text": "<p>kubeadm is a popular option for creating kubernetes clusters. kubeadm has configuration options to specify configuration information for cloud providers. For example a typical in-tree cloud provider can be configured using kubeadm as shown below:</p>",
        "image": "../assets/image/itCloud/kubernetes/kubeadmin.PNG",
        "imageClass": "mx-auto d-block"
      },
      {
        "text": "<p><b>Notes: </b><ul><li><code>kubeadm upgrade apply v1.11.0 --feature-gates=CoreDNS=true</code> - upgrade an existing cluster with kubeadm.</li><ul></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 2200,
    "date": "2020/03/14",
    "title": "Headless Services",
    "itemList": [
      {
        "text": "<p>Sometimes you don’t need load-balancing and a single Service IP. In this case, you can create what are termed “headless” Services, by explicitly specifying 'None' for the cluster IP (.spec.clusterIP).</p><p>You can use a headless Service to interface with other service discovery mechanisms, without being tied to Kubernetes’ implementation.</p><p>For headless Services, a cluster IP is not allocated, kube-proxy does not handle these Services, and there is no load balancing or proxying done by the platform for them. How DNS is automatically configured depends on whether the Service has selectors defined.</p>",
        "image": ""
      },
      {
        "text": "<p><b>With selectors</b></p><p>For headless Services that define selectors, the endpoints controller creates Endpoints records in the API, and modifies the DNS configuration to return records (addresses) that point directly to the Pods backing the Service.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Without selectors</b></p><p>For headless Services that do not define selectors, the endpoints controller does not create Endpoints records. However, the DNS system looks for and configures either:<ul><li>CNAME records for ExternalName-type Services.</li><li>A records for any Endpoints that share a name with the Service, for all other types.</li></ul></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 2400,
    "date": "2020/04/06",
    "title": "Master Node",
    "itemList": [
      {
        "text": "<p><b>Scaling up etcd clusters: </b></p><p>Scaling up etcd clusters increases availability by trading off performance. Scaling does not increase cluster performance nor capability. A general rule is not to scale up or down etcd clusters. Do not configure any auto scaling groups for etcd clusters. It is highly recommended to always run a static five-member etcd cluster for production Kubernetes clusters at any officially supported scale.</p><p>A reasonable scaling is to upgrade a three-member cluster to a five-member one, when more reliability is desired. See etcd Reconfiguration Documentation for information on how to add members into an existing cluster.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Sets up a HA-compatible cluster in the GCE zone europe-west1-b: </b></p><p>MULTIZONE=true KUBE_GCE_ZONE=europe-west1-b  ENABLE_ETCD_QUORUM_READS=true ./cluster/kube-up.sh</p>",
        "image": ""
      },
      {
        "text": "<p><b>Replicates the master on an existing HA-compatible cluster: </b></p><p>KUBE_GCE_ZONE=europe-west1-c KUBE_REPLICATE_EXISTING_MASTER=true ./cluster/kube-up.sh</p>",
        "image": ""
      },
      {
        "text": "<p><b>Removes a master replica from an existing HA cluster: </b></p><p>KUBE_DELETE_NODES=false KUBE_GCE_ZONE=europe-west1-c ./cluster/kube-down.sh</p>",
        "image": ""
      },
      {
        "text": "<p><b>Remove the broken replica: </b></p><p>KUBE_DELETE_NODES=false KUBE_GCE_ZONE=replica_zone KUBE_REPLICA_NAME=replica_name ./cluster/kube-down.sh</p>",
        "image": ""
      },
      {
        "text": "<p><b>Best practices for replicating masters for HA clusters</b><p><p><ul><li>Try to place master replicas in different zones. During a zone failure, all masters placed inside the zone will fail. To survive zone failure, also place nodes in multiple zones (see multiple-zones for details).</li><li>Do not use a cluster with two master replicas. Consensus on a two-replica cluster requires both replicas running when changing persistent state. As a result, both replicas are needed and a failure of any replica turns cluster into majority failure state. A two-replica cluster is thus inferior, in terms of HA, to a single replica cluster.</li><li>When you add a master replica, cluster state (etcd) is copied to a new instance. If the cluster is large, it may take a long time to duplicate its state. This operation may be sped up by migrating etcd data directory, as described here (we are considering adding support for etcd data dir migration in future).</li></ul></p>",
        "image": "../assets/image/itCloud/kubernetes/masterCluster.png",
        "imageClass": "mx-auto d-block"
      }
    ]
  },
  {
    "id": 2500,
    "date": "2020/05/07",
    "title": "K8s Extensions",
    "itemList": [
      {
        "text": "<p><b>Service Catalog</b></p><p>Service Catalog is an extension API that enables applications running in Kubernetes clusters to easily use external managed software offerings, such as a datastore service offered by a cloud provider. The following diagram depicts the architecture of the Service Catalog:</p>",
        "image": "../assets/image/itCloud/kubernetes/serviceCatalog.PNG",
        "imageClass": "mx-auto d-block"
      },
      {
        "text": "<p>If you are familiar with Helm Charts, install Service Catalog using Helm into your Kubernetes cluster. Alternatively, you can install Service Catalog using the SC tool.</p>",
        "image": ""
      }
    ]
  },
  {
    "id": 2600,
    "date": "2020/05/07",
    "title": "Large Clusters and Performance",
    "itemList": [
      {
        "text": "<p>At v1.18, Kubernetes supports clusters with up to 5000 nodes. More specifically, we support configurations that meet all of the following criteria:</p>",
        "image": ""
      },
      {
        "text": "<p><ul><li>No more than 5000 nodes</li><li>No more than 150000 total pods</li><li>No more than 300000 total containers</li><li>No more than 100 pods per node</li></ul></p>",
        "image": ""
      },
      {
        "text": "<p>When setting up a large Kubernetes cluster, the following issues must be considered.<ul><li>Quota Issues</li><li>Etcd storage</li><li>Size of master and master components</li><li>Addon Resources Limits</li><li>Allowing minor node failure at startup</li></ul></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 9000,
    "date": "2019/12/19",
    "title": "Notes",
    "itemList": [
      {
        "text": "<p><b>Notes 1: </b><ul><li>As of Kubernetes v1.12, CoreDNS is the recommended DNS Server, replacing kube-dns.</li><li>Note that resource quota divides up aggregate cluster resources, but it creates no restrictions around nodes: pods from several namespaces may run on the same node.</li><li>Kubernetes does not directly support hybrid clusters.</li><li>You can use Kubernetes to run a mixture of Linux and Windows nodes, so you can mix Pods that run on Linux on with Pods that run on Windows. This page shows how to register Windows nodes to your cluster.</li><li>You use the <code>kubectl cluster-info</code> command to retrieve the service’s proxy URL. To create proxy URLs that include service endpoints, suffixes, and parameters, you simply append to the service’s proxy URL: http://kubernetes_master_address/api/v1/namespaces/namespace_name/services/[https:]service_name[:port_name]/proxy</li><ul></p>",
        "image": ""
      },
      {
        "text": "<p><b>Notes 2: </b><ul><li><code>sudo docker run -it --rm --privileged --net=host -v /:/rootfs -v $CONFIG_DIR:$CONFIG_DIR -v $LOG_DIR:/var/result k8s.gcr.io/node-test:0.2</code> - run the node conformance test. $CONFIG_DIR is the pod manifest path of your Kubelet. $LOG_DIR is the test output path.</li><li>Use a projected Volume to mount several existing volume sources into the same directory. Currently, secret, configMap, downwardAPI, and serviceAccountToken volumes can be projected.</li><li>Log paths in Master:<ul><li>/var/log/kube-apiserver.log - API Server, responsible for serving the API</li><li>/var/log/kube-scheduler.log - Scheduler, responsible for making scheduling decisions</li><li>/var/log/kube-controller-manager.log - Controller that manages replication controllers</li></ul></li><li>Log paths in worker nodes:<ul><li>/var/log/kube-proxy.log - Kube Proxy, responsible for service load balancing</li><li>/var/log/kubelet.log - Kubelet, responsible for running containers on the node</li></ul></li><ul></p>",
        "image": ""
      },
      {
        "text": "<p><b>Kubernetes Architecture Diagram</b></p>",
        "image": "../assets/image/itCloud/kubernetes/k8sDiagram.png",
        "imageClass": "mx-auto d-block"
      },
      {
        "text": "<p><b>Kubernetes Concept Diagram</b></p>",
        "image": "../assets/image/itCloud/kubernetes/k8sConcept.png",
        "imageClass": "mx-auto d-block"
      }
    ]
  },
  {
    "id": 10000,
    "date": "2020/01/31",
    "title": "Interview",
    "itemList": [
      {
        "text": "<ol><li><b>What are Authentication modules for accessing Kubernetes API?</b><p>Authentication modules include Client Certificates, Password, and Plain Tokens, Bootstrap Tokens, and JWT Tokens (used for service accounts). Multiple authentication modules can be specified, in which case each one is tried in sequence, until one of them succeeds.</li><li><b>Kubernetes Principles: </b><ul><li>Declarative over Imperative</li><li>No hidden internal APIs</li><li><b>Meet the user where they are</b>: Meaning Kubernetes should not require an application to be re-rewritten to run on Kubernetes. Many applications, for example, accept secrets and config info as files or environment variables. Therefore, Kubernetes supports injecting secrets and config maps into pods as files or environment variables.</li><li>Workload portability</li></ul></li><li><b>Readiness probe vs Liveness probe?</b><p>Kubernetes uses liveness probes to know when to restart a container. Kubernetes uses readiness probes to decide when the container is available for accepting traffic. While process health checks and liveness checks are intended to recover from the failure by restarting the container, the readiness check buys time for your application and expects it to recover by itself by removing the old one and create a new one.</p><p>In many cases, you have liveness and readiness probes performing the same checks. However, the presence of a readiness probe gives your container time to start up. Only by passing the readiness check is a Deployment considered to be successful, so that, for example, Pods with an older version can be terminated as part of a rolling update.</p><p>The liveness and readiness probes are fundamental building blocks in the automation of cloud-native applications. Application frameworks such as Spring actuator, Wild‐Fly Swarm health check, Karaf health checks, or the MicroProfile spec for Java provide implementations for offering Health Probes.</p></li><li><b>Deployments vs StatefulSets vs DaemonSets</b><p>Deployment is the easiest and most used resource for deploying your application. It is a Kubernetes controller that matches the current state of your cluster to the desired state mentioned in the Deployment manifest. e.g. If you create a deployment with 1 replica, it will check that the desired state of ReplicaSet is 1 and current state is 0, so it will create a ReplicaSet, which will further create the pod. If you create a deployment with name counter, it will create a ReplicaSet with name counter-<replica-set-id>, which will further create a Pod with name counter-<replica-set->-<pod-id>.</p><p>Deployments are usually used for stateless applications. However, you can save the state of deployment by attaching a Persistent Volume to it and make it stateful, but all the pods of a deployment will be sharing the same Volume and data across all of them will be same.</p><p>StatefulSet(stable-GA in k8s v1.9) is a Kubernetes resource used to manage stateful applications. It manages the deployment and scaling of a set of Pods, and provides guarantee about the ordering and uniqueness of these Pods.</p><p>StatefulSet is also a Controller but unlike Deployments, it doesn’t create ReplicaSet rather itself creates the Pod with a unique naming convention. e.g. If you create a StatefulSet with name counter, it will create a pod with name counter-0, and for multiple replicas of a statefulset, their names will increment like counter-0, counter-1, counter-2, etc</p><p>A DaemonSet is a controller that ensures that the pod runs on all the nodes of the cluster. If a node is added/removed from a cluster, DaemonSet automatically adds/deletes the pod.</p><p>However, Daemonset automatically doesn’t run on nodes which have a taint e.g. Master. You will have to specify the tolerations for it on the pod.</p></li><li><b>Quality of Services Classes</b><p>When Kubernetes creates a Pod it assigns one of these QoS classes to the Pod:<ul><li>Guaranteed</li><li>Burstable</li><li>BestEffort</li></ul></p></li><li><b>What is Downward API?</b><p>It is sometimes useful for a Container to have information about itself, without being overly coupled to Kubernetes. The Downward API allows containers to consume information about themselves or the cluster without using the Kubernetes client or API server.</p><p>An example is an existing application that assumes a particular well-known environment variable holds a unique identifier. One possibility is to wrap the application, but that is tedious and error prone, and it violates the goal of low coupling. A better option would be to use the Pod’s name as an identifier, and inject the Pod’s name into the well-known environment variable.</p></ol>",
        "image": ""
      }
    ]
  }
]