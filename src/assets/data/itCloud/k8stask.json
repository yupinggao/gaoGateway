[
  {
    "id": 50,
    "date": "2020/05/16",
    "title": "Access Clusters Using the K8s API",
    "itemList": [
      {
        "text": "<p><b>Using kubectl proxy</b><br><code>kubectl proxy --port=8080 &</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>curl http://localhost:8080/api/</code></p>",
        "image": ""
      },
      {
        "text": "<p><b>Without kubectl proxy</b><br><code>kubectl config view -o jsonpath='{'Cluster name\tServer\\n'}{range .clusters[*]}{.name}{'\\t'}{.cluster.server}{'\\n'}{end}'</code> - Check all possible clusters, as your .KUBECONFIG may have multiple contexts</p>",
        "image": ""
      },
      {
        "text": "<p><code>export CLUSTER_NAME='gke_myk8sproject-263317_us-central1-a_temp-c'</code> - view container log</p>",
        "image": ""
      },
      {
        "text": "<p><code>APISERVER=$(kubectl config view -o jsonpath='{.clusters[?(@.name==\\'$CLUSTER_NAME\\')].cluster.server}')</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>TOKEN=$(kubectl get secrets -o jsonpath=\\{.items[?(@.metadata.annotations['kubernetes\\.io/service-account\\.name']=='default')].data.token}'|base64 --decode)</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>curl -X GET $APISERVER/api --header 'Authorization: Bearer $TOKEN' --insecure</code></p>",
        "image": ""
      },
      {
        "text": "<p><b>Programmatic access to the API</b></p>",
        "image": ""
      },
      {
        "text": "<p>Kubernetes officially supports client libraries for Go, Python, Java, dotnet, Javascript, and Haskell. There are other client libraries that are provided and maintained by their authors, not the Kubernetes team. See client libraries for accessing the API from other languages and how they authenticate.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Accessing the API from within a Pod</b></p><p>The easiest way to use the Kubernetes API from a Pod is to use one of the official client libraries. These libraries can automatically discover the API server and authenticate.</p>",
        "image": ""
      },
      {
        "text": "<p><ul><li>Using Official Client Libraries</li><li>Directly accessing the REST API</li><li>Using kubectl proxy</li><li>Without using a proxy<p><code></code><br><code>APISERVER=https://kubernetes.default.svc</code><br><code>SERVICEACCOUNT=/var/run/secrets/kubernetes.io/serviceaccount</code><br><code>NAMESPACE=$(cat ${SERVICEACCOUNT}/namespace)</code><br><code>TOKEN=$(cat ${SERVICEACCOUNT}/token)</code><br><code>CACERT=${SERVICEACCOUNT}/ca.crt</code><br><code>curl --cacert ${CACERT} --header 'Authorization: Bearer ${TOKEN}' -X GET ${APISERVER}/api</code></p></li></ul></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 100,
    "date": "2020/05/08",
    "title": "Exploring App",
    "itemList": [
      {
        "text": "<p><code>kubectl proxy</code> - note: in a new terminal</p>",
        "image": ""
      },
      {
        "text": "<p><code>export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{'\\n'}}{{end}}') echo Name of the Pod: $POD_NAME</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>curl http://localhost:8001/api/v1/namespaces/default/pods/$POD_NAME/proxy/</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl logs $POD_NAME</code> - view container log</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl exec $POD_NAME envs</code> - list the environment variables</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl exec -ti $POD_NAME bash</code> - start a bash session in the Pod’s container</p>",
        "image": ""
      },
      {
        "text": "<p><code>cat server.js</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>curl localhost:8080</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>exit</code></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 200,
    "date": "2020/05/08",
    "title": "Exposing App",
    "itemList": [
      {
        "text": "<p><code>kubectl expose deployment/kubernetes-bootcamp --type='NodePort' --port 8080</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl describe services/kubernetes-bootcamp</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>export NODE_PORT=$(kubectl get services/kubernetes-bootcamp -o go-template='{{(index .spec.ports 0).nodePort}}') echo NODE_PORT=$NODE_PORT</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>curl $(minikube ip):$NODE_PORT</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get pods -l run=kubernetes-bootcamp</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get services -l run=kubernetes-bootcamp</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{'\\n'}}{{end}}') echo Name of the Pod: $POD_NAME</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl label pod $POD_NAME app=v1</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl delete service -l run=kubernetes-bootcamp</code></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 300,
    "date": "2020/05/08",
    "title": "Scale App",
    "itemList": [
      {
        "text": "<p><code>kubectl get deployments</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get rs</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl scale deployments/kubernetes-bootcamp --replicas=4</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get deployments</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get pods -o wide</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl describe deployments/kubernetes-bootcamp</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl describe services/kubernetes-bootcamp</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>export NODE_PORT=$(kubectl get services/kubernetes-bootcamp -o go-template='{{(index .spec.ports 0).nodePort}}') echo NODE_PORT=$NODE_PORT</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>curl $(minikube ip):$NODE_PORT</code></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 400,
    "date": "2020/05/08",
    "title": "Rolling Update",
    "itemList": [
      {
        "text": "<p><code>kubectl get deployments</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get pods</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl describe pods</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v2</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get pods</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl describe services/kubernetes-bootcamp</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>export NODE_PORT=$(kubectl get services/kubernetes-bootcamp -o go-template='{{(index .spec.ports 0).nodePort}}') echo NODE_PORT=$NODE_PORT</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>curl $(minikube ip):$NODE_PORT</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl rollout status deployments/kubernetes-bootcamp</code> - see the Deployment rollout status</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl describe pods</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=gcr.io/google-samples/kubernetes-bootcamp:v10</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get deployments</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get pods</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl describe pods</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl rollout undo deployments/kubernetes-bootcamp</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get pods</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl describe pods</code></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 500,
    "date": "2020/05/09",
    "title": "Redis using a ConfigMap",
    "itemList": [
      {
        "text": "<p><code>gcloud container clusters create temp-c --num-nodes=1 --zone us-central1-a</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>curl -OL https://k8s.io/examples/pods/config/redis-config</code></p>",
        "image": "../assets/image/itCloud/k8sTask/1.PNG"
      },
      {
        "text": "<p><code>curl -OL https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/pods/config/redis-pod.yaml</code></p>",
        "image": "../assets/image/itCloud/k8sTask/2.PNG"
      },
      {
        "text": "<p><code>kubectl apply -k .</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get -k .</code> - examine the created objects</p>",
        "image": ""
      },
      {
        "text": "<p><code>In the example, the config volume is mounted at /redis-master. It uses path to add the redis-config key to a file named redis.conf. The file path for the redis config, therefore, is /redis-master/redis.conf. This is where the image will look for the config file for the redis master.</code></p><p>Use kubectl exec to enter the pod and run the redis-cli tool to verify that the configuration was correctly applied:</p>",
        "image": "../assets/image/itCloud/k8sTask/3.PNG"
      },
      {
        "text": "<p><code>kubectl exec -it redis bash</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>cd /redis-master</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>cat redis.conf</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl delete pod redis</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>gcloud container clusters delete temp-c --zone us-central1-a</code></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 600,
    "date": "2020/05/09",
    "title": "Exposing an External IP",
    "itemList": [
      {
        "text": "<p><code>kubectl apply -f load-balancer-example.yaml</code></p>",
        "image": "../assets/image/itCloud/k8sTask/4.PNG"
      },
      {
        "text": "<p><code>kubectl get deployments hello-world</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl describe deployments hello-world</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get replicasets</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl describe replicasets</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl expose deployment hello-world --type=LoadBalancer --name=my-service</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get services my-service</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl describe services my-service</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get pods --output=wide</p>",
        "image": ""
      },
      {
        "text": "<p>http://external-ip:port. e.g. http://35.226.116.169:8080</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl delete services my-service</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl delete deployment hello-world</p>",
        "image": ""
      }
    ]
  },
  {
    "id": 700,
    "date": "2020/05/09",
    "title": "Application with Redis",
    "itemList": [
      {
        "text": "<p><code>gcloud container clusters resize temp-c --zone us-central1-a --num-nodes 2</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-deployment.yaml</code></p>",
        "image": "../assets/image/itCloud/k8sTask/5.PNG"
      },
      {
        "text": "<p><code>kubectl get pods</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl logs -f redis-master-596696dd4-bhxcq</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-service.yaml</code></p>",
        "image": "../assets/image/itCloud/k8sTask/6.PNG"
      },
      {
        "text": "<p><code>kubectl get service</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl apply -f https://k8s.io/examples/application/guestbook/redis-slave-deployment.yaml</code></p>",
        "image": "../assets/image/itCloud/k8sTask/7.PNG"
      },
      {
        "text": "<p><code>kubectl get pods</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl apply -f https://k8s.io/examples/application/guestbook/redis-slave-service.yaml</p>",
        "image": "../assets/image/itCloud/k8sTask/8.PNG"
      },
      {
        "text": "<p><code>kubectl get services</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl apply -f https://k8s.io/examples/application/guestbook/frontend-deployment.yaml</p>",
        "image": "../assets/image/itCloud/k8sTask/9.PNG"
      },
      {
        "text": "<p><code>kubectl get pods -l app=guestbook -l tier=frontend</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl apply -f frontend-service-lb.yaml</code></p>",
        "image": "../assets/image/itCloud/k8sTask/10.PNG"
      },
      {
        "text": "<p><code>kubectl get service frontend-lb</code></p>",
        "image": ""
      },
      {
        "text": "<p>http://external-ip:port. e.g. http://35.188.81.39:80</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl scale deployment frontend --replicas=2</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get pods</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl delete deployment -l app=redis</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl delete service -l app=redis</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl delete deployment -l app=guestbook</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl delete service -l app=guestbook</code></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 800,
    "date": "2020/05/10",
    "title": "Logging and Metrics",
    "itemList": [
      {
        "text": "<p>This example consists of the following components:<ol><li>A running instance of the PHP Guestbook with Redis tutorial - see the last Task</li><li>Elasticsearch and Kibana</li><li>Filebeat</li><li>Metricbeat</li><li>Packetbeat</li></ol></p>",
        "image": ""
      },
      {
        "text": "<p>Create a cluster level role binding so that you can deploy kube-state-metrics and the Beats at the cluster level (in kube-system)<br><code>kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=yangxigao99@gmail.com</code></p>",
        "image": ""
      },
      {
        "text": "<p>Check if kube-state-metrics is running:<br><code>kubectl get pods --namespace=kube-system | grep kube-state</code></p>",
        "image": ""
      },
      {
        "text": "<p>Install it if not:<br><code>git clone https://github.com/kubernetes/kube-state-metrics.git kube-state-metric</code><br><code>kubectl apply -f kube-state-metric/examples/standard</code><br><code>kubectl get pods --namespace=kube-system | grep kube-state</code></p>",
        "image": ""
      },
      {
        "text": "<p>Verify that kube-state-metrics is running and ready:<br><code>kubectl get pods -n kube-system -l app.kubernetes.io/name=kube-state-metrics</code></p>",
        "image": ""
      },
      {
        "text": "<p>Clone the Elastic examples GitHub repo:<br><code>git clone https://github.com/elastic/examples.git</code><br><code>cd examples/beats-k8s-send-anywhere</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>nano ELASTIC_CLOUD_ID</code> - elastic cloud service ID</p>",
        "image": ""
      },
      {
        "text": "<p><code>nano ELASTIC_CLOUD_AUTH</code> - elastic userName:Password</p>",
        "image": ""
      },
      {
        "text": "<p>Create the secret:<br><code>kubectl create secret generic dynamic-logging --from-file=./ELASTIC_CLOUD_ID --from-file=./ELASTIC_CLOUD_AUTH --namespace=kube-system</code></p>",
        "image": ""
      },
      {
        "text": "<p>Deploy the beats:<br><code>kubectl create -f filebeat-kubernetes.yaml</code></p>",
        "image": "../assets/image/itCloud/k8sTask/11.PNG"
      },
      {
        "text": "Verify:<br><p><code>kubectl get pods -n kube-system -l k8s-app=filebeat-dynamic</p>",
        "image": ""
      },
      {
        "text": "<p>Deploy Metricbeat:<br><code>kubectl create -f metricbeat-kubernetes.yaml</code></p>",
        "image": "../assets/image/itCloud/k8sTask/12.PNG"
      },
      {
        "text": "Verify: <br><p><code>kubectl get pods -n kube-system -l k8s-app=metricbeat</p>",
        "image": ""
      },
      {
        "text": "<p>Deploy Packetbeat:<br><code>kubectl create -f packetbeat-kubernetes.yaml</code></p>",
        "image": "../assets/image/itCloud/k8sTask/13.PNG"
      },
      {
        "text": "<p>Verify:<br><code>kubectl get pods -n kube-system -l k8s-app=packetbeat-dynamic</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl delete deployment -l app=redis</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl delete service -l app=redis<code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl delete deployment -l app=guestbook</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl delete service -l app=guestbook</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl delete -f filebeat-kubernetes.yaml</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl delete -f metricbeat-kubernetes.yaml</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl delete -f packetbeat-kubernetes.yaml</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl delete secret dynamic-logging -n kube-system</code></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 900,
    "date": "2020/05/11",
    "title": "StatefulSet",
    "itemList": [
      {
        "text": "<p><br><code>kubectl apply -f web.yaml</code></p>",
        "image": "../assets/image/itCloud/k8sTask/14.PNG"
      },
      {
        "text": "<p><code>kubectl get service nginx</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get statefulset web</code></p>",
        "image": ""
      },
      {
        "text": "<p>Each Pod has a stable hostname based on its ordinal index. Use kubectl exec to execute the hostname command in each Pod:<br><code>for i in 0 1; do kubectl exec web-$i -- sh -c 'hostname'; done</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm</p>",
        "image": ""
      },
      {
        "text": "<p><code>nslookup web-0.nginx</code> - using nslookup on the Pods’ hostnames, you can examine their in-cluster DNS addresses</p>",
        "image": ""
      },
      {
        "text": "<p><code>nslookup web-1.nginx</code></p>",
        "image": ""
      },
      {
        "text": "<p>In one terminal, watch the StatefulSet’s Pods:<br><code>kubectl get pod -w -l app=nginx</code></p>",
        "image": ""
      },
      {
        "text": "<p>In a second terminal, use kubectl delete to delete all the Pods in the StatefulSet:<br><code>kubectl delete pod -l app=nginx</code></p>",
        "image": ""
      },
      {
        "text": "Wait for the StatefulSet to restart them, and for both Pods to transition to Running and Ready:<br><p><code>kubectl get pod -w -l app=nginx</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>for i in 0 1; do kubectl exec web-$i -- sh -c 'hostname'; done</code></p>",
        "image": ""
      },
      {
        "text": "<br><p><code>kubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm /bin/sh</p>",
        "image": ""
      },
      {
        "text": "<p><code>nslookup web-0.nginx</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>nslookup web-1.nginx</code></p>",
        "image": ""
      },
      {
        "text": "<p>The Pods’ ordinals, hostnames, SRV records, and A record names have not changed, but the IP addresses associated with the Pods may have changed. In the cluster used for this tutorial, they have. This is why it is important not to configure other applications to connect to Pods in a StatefulSet by IP address.</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get pvc -l app=nginx</code> - The StatefulSet controller created two PersistentVolumeClaims that are bound to two PersistentVolumes. As the cluster used in this tutorial is configured to dynamically provision PersistentVolumes, the PersistentVolumes were created and bound automatically.</p>",
        "image": ""
      },
      {
        "text": "<p>Write the Pods’ hostnames to their index.html files and verify that the NGINX webservers serve the hostnames:<br><code>for i in 0 1; do kubectl exec web-$i -- sh -c 'echo $(hostname) > /usr/share/nginx/html/index.html'; done<code></p>",
        "image": ""
      },
      {
        "text": "<p><code>for i in 0 1; do kubectl exec -it web-$i -- curl localhost; done</code></p>",
        "image": ""
      },
      {
        "text": "<p>If you instead see 403 Forbidden responses for the above curl command, you will need to fix the permissions of the directory mounted by the volumeMounts (due to a bug when using hostPath volumes) with:<br><code>for i in 0 1; do kubectl exec web-$i -- chmod 755 /usr/share/nginx/html; done</code></p>",
        "image": ""
      },
      {
        "text": "<p>In one terminal, watch the StatefulSet’s Pods:<br><code>kubectl get pod -w -l app=nginx</code></p>",
        "image": ""
      },
      {
        "text": "<p>In a second terminal, delete all of the StatefulSet’s Pods:<br><code>kubectl delete pod -l app=nginx</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get pod -w -l app=nginx</code></p>",
        "image": ""
      },
      {
        "text": "<p>Verify the web servers continue to serve their hostnames:<br><code>for i in 0 1; do kubectl exec -it web-$i -- curl localhost; done</code></p>",
        "image": ""
      },
      {
        "text": "<p>Even though web-0 and web-1 were rescheduled, they continue to serve their hostnames because the PersistentVolumes associated with their PersistentVolumeClaims are remounted to their volumeMounts. No matter what node web-0and web-1 are scheduled on, their PersistentVolumes will be mounted to the appropriate mount points.</code></p>",
        "image": ""
      },
      {
        "text": "<p>scale the number of replicas to 5:<br><code>kubectl scale sts web --replicas=5</code></p>",
        "image": ""
      },
      {
        "text": "<p>In another terminal, use kubectl patch to scale the StatefulSet back down to three replicas:<br><code>kubectl patch sts web -p '{'spec':{'replicas':3}}'</code></p>",
        "image": ""
      },
      {
        "text": "<p>Get the StatefulSet’s PersistentVolumeClaims:<br><code>kubectl get pvc -l app=nginx</code></p>",
        "image": ""
      },
      {
        "text": "<p>There are still five PersistentVolumeClaims and five PersistentVolumes. When exploring a Pod’s stable storage, we saw that the PersistentVolumes mounted to the Pods of a StatefulSet are not deleted when the StatefulSet’s Pods are deleted. This is still true when Pod deletion is caused by scaling the StatefulSet down.</code></p>",
        "image": ""
      },
      {
        "text": "<p>Patch the web StatefulSet to apply the RollingUpdate update strategy:<br><code>kubectl patch statefulset web -p '{'spec':{'updateStrategy':{'type':'RollingUpdate'}}}'</code></p>",
        "image": ""
      },
      {
        "text": "<p>patch the web StatefulSet to change the container image again:<br><code>kubectl patch statefulset web --type='json' -p='[{'op': 'replace', 'path': '/spec/template/spec/containers/0/image', 'value':'gcr.io/google_containers/nginx-slim:0.8'}]'</code></p>",
        "image": ""
      },
      {
        "text": "<p>Get the Pods to view their container images:<br><code>for p in 0 1 2; do kubectl get po web-$p --template '{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}'; echo; done</code></p>",
        "image": ""
      },
      {
        "text": "<p>Patch the web StatefulSet to add a partition to the updateStrategy field:<br><code>kubectl patch statefulset web -p '{'spec':{'updateStrategy':{'type':'RollingUpdate','rollingUpdate':{'partition':3}}}}'</code></p>",
        "image": ""
      },
      {
        "text": "<p>Patch the StatefulSet again to change the container’s image:<br><code>kubectl patch statefulset web --type='json' -p='[{'op': 'replace', 'path': '/spec/template/spec/containers/0/image', 'value':'k8s.gcr.io/nginx-slim:0.7'}]'</code></p>",
        "image": ""
      },
      {
        "text": "<p>Delete a Pod in the StatefulSet:<br><code>kubectl delete po web-2</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get po -l app=nginx -w</code></p>",
        "image": ""
      },
      {
        "text": "<p>Get the Pods to view their container images:<br><code>kubectl get po web-2 --template '{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}'</code> - notice that, even though the update strategy is RollingUpdate the StatefulSet controller restored the Pod with its original container. This is because the ordinal of the Pod is less than the partition specified by the updateStrategy.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Rolling Out a Canary</b><br>Patch the StatefulSet to decrement the partition:<br><code>kubectl patch statefulset web -p '{'spec':{'updateStrategy':{'type':'RollingUpdate','rollingUpdate':{'partition':2}}}}'</code></p>",
        "image": ""
      },
      {
        "text": "<p>Get the Pod’s container:<br><code>kubectl get po web-2 --template '{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}'</code></p>",
        "image": ""
      },
      {
        "text": "<p>Delete the web-1 Pod:<br><code>kubectl delete po web-1</code></p>",
        "image": ""
      },
      {
        "text": "<p>Get the web-1 Pods container:<br><code>kubectl get po web-1 --template '{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}'</code> - web-1 was restored to its original configuration because the Pod’s ordinal was less than the partition. When a partition is specified, all Pods with an ordinal that is greater than or equal to the partition will be updated when the StatefulSet’s .spec.template is updated. If a Pod that has an ordinal less than the partition is deleted or otherwise terminated, it will be restored to its original configuration.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Phased Roll Outs</b><br>You can perform a phased roll out (e.g. a linear, geometric, or exponential roll out) using a partitioned rolling update in a similar manner to how you rolled out a canary. To perform a phased roll out, set the partition to the ordinal at which you want the controller to pause the update.<br>Set the partition to 0:<br><code>kubectl patch statefulset web -p '{'spec':{'updateStrategy':{'type':'RollingUpdate','rollingUpdate':{'partition':0}}}}'</code></p>",
        "image": ""
      },
      {
        "text": "<p>Get the Pod’s container:<br><code>for p in 0 1 2; do kubectl get po web-$p --template '{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}'; echo; done</code> - By moving the partition to 0, you allowed the StatefulSet controller to continue the update process.</p>",
        "image": ""
      },
      {
        "text": "<p><b>On Delete</b><br>The OnDelete update strategy implements the legacy (1.6 and prior) behavior, When you select this update strategy, the StatefulSet controller will not automatically update Pods when a modification is made to the StatefulSet’s .spec.template field. This strategy can be selected by setting the .spec.template.updateStrategy.type to OnDelete.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Deleting StatefulSets</b><br>StatefulSet supports both Non-Cascading and Cascading deletion. In a Non-Cascading Delete, the StatefulSet’s Pods are not deleted when the StatefulSet is deleted. In a Cascading Delete, both the StatefulSet and its Pods are deleted.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Non-Cascading Delete</b><br>In one terminal window, watch the Pods in the StatefulSet<br>:<code>kubectl get pods -w -l app=nginx</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get pods -l app=nginx</code> - Even though web has been deleted, all of the Pods are still Running and Ready.</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl delete pod web-0</code> - As the web StatefulSet has been deleted, web-0 has not been relaunched.</p>",
        "image": ""
      },
      {
        "text": "<p>Let’s take another look at the contents of the index.html file served by the Pods’ webservers:<br><code>for i in 0 1; do kubectl exec -it web-$i -- curl localhost; done</code> - Even though you deleted both the StatefulSet and the web-0 Pod, it still serves the hostname originally entered into its index.html file. This is because the StatefulSet never deletes the PersistentVolumes associated with a Pod. When you recreated the StatefulSet and it relaunched web-0, its original PersistentVolume was remounted.</p>",
        "image": ""
      },
      {
        "text": "<p>Delete the StatefulSet again. This time, omit the --cascade=false parameter:<br><code>kubectl delete statefulset web</code> - As you saw in the Scaling Down section, the Pods are terminated one at a time, with respect to the reverse order of their ordinal indices. Before terminating a Pod, the StatefulSet controller waits for the Pod’s successor to be completely terminated.</p>",
        "image": ""
      },
      {
        "text": "<p>Note that, while a cascading delete will delete the StatefulSet and its Pods, it will not delete the Headless Service associated with the StatefulSet. You must delete the nginx Service manually:<br><code>kubectl delete service nginx</code></p>",
        "image": ""
      },
      {
        "text": "<p>Recreate the StatefulSet and Headless Service one more time:<br><code>kubectl apply -f web.yaml</code> - As the web StatefulSet has been deleted, web-0 has not been relaunched.</p>",
        "image": ""
      },
      {
        "text": "<p>When all of the StatefulSet’s Pods transition to Running and Ready, retrieve the contents of their index.html files:<br><code>for i in 0 1; do kubectl exec -it web-$i -- curl localhost; done</code> - Even though you completely deleted the StatefulSet, and all of its Pods, the Pods are recreated with their PersistentVolumes mounted, and web-0 and web-1 will still serve their hostnames.</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl delete service nginx</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl delete statefulset web</code></p>",
        "image": ""
      },
      {
        "text": "<p><b>OrderedReady Pod Management and Parallel Pod Management</b></p><p>OrderedReady pod management is the default for StatefulSets. It tells the StatefulSet controller to respect the ordering guarantees demonstrated above.</p><p>Parallel pod management tells the StatefulSet controller to launch or terminate all Pods in parallel, and not to wait for Pods to become Running and Ready or completely terminated prior to launching or terminating another Pod.</p><p>To use Parallel Pod Management: <code>.spec.podManagementPolicy of the web StatefulSet is set to Parallel</code></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 1000,
    "date": "2020/05/12",
    "title": "WordPress and MySQL",
    "itemList": [
      {
        "text": "<p>Add a Secret generator in kustomization.yaml:</p>",
        "image": "../assets/image/itCloud/k8sTask/15.PNG"
      },
      {
        "text": "<p><code>curl -LO https://k8s.io/examples/application/wordpress/mysql-deployment.yaml</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>curl -LO https://k8s.io/examples/application/wordpress/wordpress-deployment.yaml</code></p>",
        "image": ""
      },
      {
        "text": "<p>Add them to kustomization.yaml file:</p>",
        "image": "../assets/image/itCloud/k8sTask/16.PNG"
      },
      {
        "text": "<p><code>kubectl apply -k ./</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get secrets</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get pvc</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get pods</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get services wordpress</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>http://35.239.147.208:80/</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl delete -k ./</code></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 1100,
    "date": "2020/05/12",
    "title": "Deploying Cassandra",
    "itemList": [
      {
        "text": "<p><code>kubectl apply -f https://k8s.io/examples/application/cassandra/cassandra-service.yaml</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl apply -f https://k8s.io/examples/application/cassandra/cassandra-statefulset.yaml</code></p>",
        "image": ""
      },
      {
        "text": "<p>Run the Cassandra nodetool inside the first Pod, to display the status of the ring:<br><code>kubectl exec -it cassandra-0 -- nodetool status</code></p>",
        "image": ""
      },
      {
        "text": "<p>Run the following commands (chained together into a single command) to delete everything in the Cassandra StatefulSet:</p>",
        "image": "../assets/image/itCloud/k8sTask/17.PNG"
      },
      {
        "text": "<p><code>kubectl delete service -l app=cassandra</code></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 1200,
    "date": "2020/05/12",
    "title": "Running ZooKeeper",
    "itemList": [
      {
        "text": "<p><code>gcloud container clusters create hello-cluster --enable-autoscaling --max-nodes=6 --min-nodes=1 --zone us-central1-a</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl apply -f https://k8s.io/examples/application/zookeeper/zookeeper.yaml</code></p>",
        "image": ""
      },
      {
        "text": "<p>Use kubectl exec to get the hostnames of the Pods in the zk StatefulSet:<br><code>for i in 0 1 2; do kubectl exec zk-$i -- hostname; done</code></p>",
        "image": ""
      },
      {
        "text": "<p>The servers in a ZooKeeper ensemble use natural numbers as unique identifiers, and store each server’s identifier in a file called myid in the server’s data directory. To examine the contents of the myid file for each server use the following command:<br><code></code></p>",
        "image": ""
      },
      {
        "text": "<p>To get the Fully Qualified Domain Name (FQDN) of each Pod in the zk StatefulSet use the following command:<br><code>for i in 0 1 2; do kubectl exec zk-$i -- hostname -f; done</code></p>",
        "image": ""
      },
      {
        "text": "<p>ZooKeeper stores its application configuration in a file named zoo.cfg. Use kubectl exec to view the contents of the zoo.cfg file in the zk-0 Pod:<br><code>kubectl exec zk-0 -- cat /opt/zookeeper/conf/zoo.cfg</code></p>",
        "image": ""
      },
      {
        "text": "<p><b>Sanity Testing the Ensemble</b><br>The most basic sanity test is to write data to one ZooKeeper server and to read the data from another.</p>",
        "image": ""
      },
      {
        "text": "<p>The command below executes the zkCli.sh script to write world to the path /hello on the zk-0 Pod in the ensemble:<br><code>kubectl exec zk-0 zkCli.sh create /hello world</code></p>",
        "image": ""
      },
      {
        "text": "<p>To get the data from the zk-1 Pod use the following command:<br><code>kubectl exec zk-1 zkCli.sh get /hello</code></p>",
        "image": ""
      },
      {
        "text": "<p>Use the kubectl delete command to delete the zk StatefulSet:<br><code>kubectl delete statefulset zk</code></p>",
        "image": ""
      },
      {
        "text": "<p>Watch the termination of the Pods in the StatefulSet:<br><code>kubectl get pods -w -l app=zk</code></p>",
        "image": ""
      },
      {
        "text": "<p>Reapply the manifest in zookeeper.yaml:<br><code>kubectl apply -f https://k8s.io/examples/application/zookeeper/zookeeper.yaml</code> - This creates the zk StatefulSet object, but the other API objects in the manifest are not modified because they already exist.</p>",
        "image": ""
      },
      {
        "text": "<p>Watch the StatefulSet controller recreate the StatefulSet’s Pods:<br><code>kubectl get pods -w -l app=zk</code></p>",
        "image": ""
      },
      {
        "text": "<p>Use the command below to get the value you entered during the sanity test, from the zk-2 Pod:<br><code>kubectl exec zk-2 zkCli.sh get /hello</code> - Even though you terminated and recreated all of the Pods in the zk StatefulSet, the ensemble still serves the original value</p>",
        "image": ""
      },
      {
        "text": "<p>Use the following command to get the StatefulSet's PersistentVolumeClaims:<br><code>kubectl get pvc -l app=zk</code></p>",
        "image": ""
      },
      {
        "text": "<p><b>Configuring Logging</b><br>One of the files generated by the zkGenConfig.sh script controls ZooKeeper’s logging. ZooKeeper uses Log4j, and, by default, it uses a time and size based rolling file appender for its logging configuration.</p>",
        "image": ""
      },
      {
        "text": "<p>Use the command below to get the logging configuration from one of Pods in the zk StatefulSet:<br><code>kubectl exec zk-0 cat /usr/etc/zookeeper/log4j.properties</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl logs zk-0 --tail 20</code></p>",
        "image": ""
      },
      {
        "text": "<p>Get the ZooKeeper process information from the zk-0 Pod:<br><code>kubectl exec zk-0 -- ps -elf</code></p>",
        "image": ""
      },
      {
        "text": "<p>Use the command below to get the file permissions of the ZooKeeper data directory on the zk-0 Pod:<br><code>kubectl exec -ti zk-0 -- ls -ld /var/lib/zookeeper/data</code></p>",
        "image": ""
      },
      {
        "text": "<p>Update the number of cpus allocated to the servers:<br><code>kubectl patch sts zk --type='json' -p='[{'op': 'replace', 'path': '/spec/template/spec/containers/0/resources/requests/cpu', 'value':'0.3'}]'</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl rollout history sts/zk</code></p>",
        "image": ""
      },
      {
        "text": "<p>Use the kubectl rollout undo command to roll back the modification:<br><code>kubectl rollout undo sts/zk</code></p>",
        "image": ""
      },
      {
        "text": "<p>Use the following command to examine the process tree for the ZooKeeper server running in the zk-0 Pod:<br><code>kubectl exec zk-0 -- ps -ef</code></p>",
        "image": ""
      },
      {
        "text": "<p>In another terminal watch the Pods in the zk StatefulSet with the following command:<br><code>kubectl get pod -w -l app=zk</code></p>",
        "image": ""
      },
      {
        "text": "<p>In another terminal, terminate the ZooKeeper process in Pod zk-0 with the following command:<br><code>kubectl exec zk-0 -- pkill java</code> - If your application uses a script (such as zkServer.sh) to launch the process that implements the application’s business logic, the script must terminate with the child process. This ensures that Kubernetes will restart the application’s container when the process implementing the application’s business logic fails.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Testing for Liveness</b><br>Configuring your application to restart failed processes is not enough to keep a distributed system healthy. There are scenarios where a system’s processes can be both alive and unresponsive, or otherwise unhealthy. You should use liveness probes to notify Kubernetes that your application’s processes are unhealthy and it should restart them.<br>The Pod template for the zk StatefulSet specifies a liveness probe.</p>",
        "image": "../assets/image/itCloud/k8sTask/18.PNG"
      },
      {
        "text": "<p>The probe calls a bash script that uses the ZooKeeper ruok four letter word to test the server’s health.</p>",
        "image": "../assets/image/itCloud/k8sTask/19.PNG"
      },
      {
        "text": "<p><code>kubectl get pod -w -l app=zk</code></p>",
        "image": ""
      },
      {
        "text": "<p>In another window, using the following command to delete the zkOk.sh script from the file system of Pod zk-0:<br><code>kubectl exec zk-0 -- rm /usr/bin/zookeeper-ready</code></p>",
        "image": ""
      },
      {
        "text": "<p><b>Tolerating Node Failure</b><br>You should always provision additional capacity to allow the processes of critical systems to be rescheduled in the event of node failures. If you do so, then the outage will only last until the Kubernetes scheduler reschedules one of the ZooKeeper servers. However, if you want your service to tolerate node failures with no downtime, you should set podAntiAffinity.</p>",
        "image": ""
      },
      {
        "text": "<p>Use the command below to get the nodes for Pods in the zk StatefulSet:<br><code>for i in 0 1 2; do kubectl get pod zk-$i --template {{.spec.nodeName}}; echo ''; done</code> - All of the Pods in the zk StatefulSet are deployed on different nodes. This is because the Pods in the zk StatefulSet have a PodAntiAffinity specified (the topologyKey kubernetes.io/hostname indicates that the domain is an individual node):</p>",
        "image": "../assets/image/itCloud/k8sTask/20.PNG"
      },
      {
        "text": "<p><b>Cordon and Drain Nodes</b><br>You should always provision additional capacity to allow the processes of critical systems to be rescheduled in the event of node failures. If you do so, then the outage will only last until the Kubernetes scheduler reschedules one of the ZooKeeper servers. However, if you want your service to tolerate node failures with no downtime, you should set podAntiAffinity.</p>",
        "image": ""
      },
      {
        "text": "<p>Use this command to get the nodes in your cluster:<br><code>kubectl get nodes</code></p>",
        "image": ""
      },
      {
        "text": "<p>Use kubectl cordon to cordon all but four of the nodes in your cluster:<br><code>kubectl cordon <i>node-name</i></code></p>",
        "image": ""
      },
      {
        "text": "<p>Use this command to get the zk-pdb PodDisruptionBudget:<br><code>kubectl get pdb zk-pdb</code> - The max-unavailable field indicates to Kubernetes that at most one Pod from zk StatefulSet can be unavailable at any time.</p>",
        "image": ""
      },
      {
        "text": "<p><code>for i in 0 1 2; do kubectl get pod zk-$i --template {{.spec.nodeName}}; echo ''; done</code></p>",
        "image": ""
      },
      {
        "text": "<p>Use kubectl drain to cordon and drain the node on which the zk-0 Pod is scheduled:<br><code>kubectl drain $(kubectl get pod zk-0 --template {{.spec.nodeName}}) --ignore-daemonsets --force --delete-local-data</code> - As there are four nodes in your cluster, kubectl drain, succeeds and the zk-0 is rescheduled to another node.</p>",
        "image": ""
      },
      {
        "text": "<p>Keep watching the StatefulSet's Pods in the first terminal and drain the node on which zk-1 is scheduled:<br><code>kubectl drain $(kubectl get pod zk-1 --template {{.spec.nodeName}}) --ignore-daemonsets --force --delete-local-data</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl exec zk-0 zkCli.sh get /hello</code> - the service is still available because its PodDisruptionBudget is respected.</p>",
        "image": ""
      },
      {
        "text": "<p>Use kubectl uncordon to uncordon the first node:<br><code>kubectl uncordon gke-hello-cluster-default-pool-41ad3d55-0cms</code></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 1300,
    "date": "2020/05/13",
    "title": "Using Source IP",
    "itemList": [
      {
        "text": "<p>Applications running in a Kubernetes cluster find and communicate with each other, and the outside world, through the Service abstraction. This document explains what happens to the source IP of packets sent to different types of Services, and how you can toggle this behavior according to your needs.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Terminology</b><br><ul><li>NAT - network address translation</li><li>Source NAT - replacing the source IP on a packet; in this page, that usually means replacing with the IP address of a node</li><li>Destination NAT - replacing the destination IP on a packet; in this page, that usually means replacing with the IP address of a Pod</li><li>VIP - a virtual IP address, such as the one assigned to every Service in Kubernetes</li><li>kube-proxy - a network daemon that orchestrates Service VIP management on every node</li><ul></p>",
        "image": ""
      },
      {
        "text": "<p>Use a small nginx webserver that echoes back the source IP of requests it receives through an HTTP header:<br><code>kubectl create deployment source-ip-app --image=k8s.gcr.io/echoserver:1.4</code></p>",
        "image": ""
      },
      {
        "text": "<p><b>Source IP for Services with Type=ClusterIP</b></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get nodes</code></p>",
        "image": ""
      },
      {
        "text": "<p>ssh to the node:<br><code>gcloud beta compute ssh --zone 'us-central1-a' 'gke-temp-c-default-pool-de2ad605-k0v8' --project 'myk8sproject-263317'</code></p>",
        "image": ""
      },
      {
        "text": "<p>Packets sent to ClusterIP from within the cluster are never source NAT’d if you’re running kube-proxy in iptables mode, (the default). You can query the kube-proxy mode by fetching http://localhost:10249/proxyMode on the node where kube-proxy is running:<br><code>curl http://localhost:10249/proxyMode</code></p>",
        "image": ""
      },
      {
        "text": "<p>You can test source IP preservation by creating a Service over the source IP app:<br><code>kubectl expose deployment source-ip-app --name=clusterip --port=80 --target-port=8080</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get svc clusterip</code></p>",
        "image": ""
      },
      {
        "text": "<p>hitting the ClusterIP from a pod in the same cluster:<br><code>kubectl expose deployment source-ip-app --name=clusterip --port=80 --target-port=8080</code></p>",
        "image": ""
      },
      {
        "text": "<p>You can test source IP preservation by creating a Service over the source IP app:<br><code>kubectl run busybox -it --image=busybox --restart=Never --rm</code></p>",
        "image": ""
      },
      {
        "text": "<p>You can then run a command inside that Pod:<br><code>ip addr</code></p>",
        "image": ""
      },
      {
        "text": "<p>then use wget to query the local webserver:<br><code>wget -qO - 10.63.254.52</code></p>",
        "image": ""
      },
      {
        "text": "<p><b>Source IP for Services with Type=NodePort</b></p>",
        "image": ""
      },
      {
        "text": "<p>Packets sent to Services with Type=NodePort are source NAT’d by default. You can test this by creating a NodePort Service:</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl expose deployment source-ip-app --name=nodeport --port=80 --target-port=8080 --type=NodePort</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>NODEPORT=$(kubectl get -o jsonpath='{.spec.ports[0].nodePort}' services nodeport)</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>NODES=$(kubectl get nodes -o jsonpath='{ $.items[*].status.addresses[?(@.type=='ExternalIP')].address }')</code></p>",
        "image": ""
      },
      {
        "text": "<p>If you’re running on a cloud provider, you may need to open up a firewall-rule for the nodes:nodeport reported above. Now you can try reaching the Service from outside the cluster through the node port allocated above:<br><code>for node in $NODES; do curl -s $node:$NODEPORT | grep -i client_address; done</code></p>",
        "image": ""
      },
      {
        "text": "<p>Set the service.spec.externalTrafficPolicy field as follows:<br><code>kubectl patch svc nodeport -p '{'spec':{'externalTrafficPolicy':'Local'}}'</code></p>",
        "image": ""
      },
      {
        "text": "<p>Now, re-run the test:<br><code>for node in $NODES; do curl --connect-timeout 1 -s $node:$NODEPORT | grep -i client_address; done</code> - Note that you only got one reply, with the right client IP, from the one node on which the endpoint pod is running.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Source IP for Services with Type=LoadBalancer</b></p>",
        "image": ""
      },
      {
        "text": "<p>Packets sent to Services with Type=LoadBalancer are source NAT’d by default, because all schedulable Kubernetes nodes in the Ready state are eligible for load-balanced traffic. So if packets arrive at a node without an endpoint, the system proxies it to a node with an endpoint, replacing the source IP on the packet with the IP of the node (as described in the previous section). You can test this by exposing the source-ip-app through a load balancer:</p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl expose deployment source-ip-app --name=loadbalancer --port=80 --target-port=8080 --type=LoadBalancer</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl get svc loadbalancer</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>curl 35.232.161.107</code></p>",
        "image": ""
      },
      {
        "text": "<p>However, if you’re running on Google Kubernetes Engine/GCE, setting the same service.spec.externalTrafficPolicy field to Local forces nodes without Service endpoints to remove themselves from the list of nodes eligible for loadbalanced traffic by deliberately failing health checks.<br><code>kubectl patch svc loadbalancer -p '{'spec':{'externalTrafficPolicy':'Local'}}'</code></p>",
        "image": ""
      },
      {
        "text": "<p>You should immediately see the service.spec.healthCheckNodePort field allocated by Kubernetes:<br><code>kubectl get svc loadbalancer -o yaml | grep -i healthCheckNodePort</code></p>",
        "image": ""
      },
      {
        "text": "<p>The service.spec.healthCheckNodePort field points to a port on every node serving the health check at /healthz. You can test this:<br><code>kubectl get pod -o wide -l run=source-ip-app</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>curl localhost:32122/healthz</code></p>",
        "image": ""
      },
      {
        "text": "<p>On another node:<br><code>curl localhost:32122/healthz</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>curl 203.0.113.140</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl delete svc -l run=source-ip-app</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl delete deployment source-ip-app</code></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 1400,
    "date": "2020/05/18",
    "title": "Extend Resources for a Node",
    "itemList": [
      {
        "text": "<p><code>kubectl get nodes</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl proxy</code> in another terminal</p>",
        "image": ""
      },
      {
        "text": "<p><code>curl --header 'Content-Type: application/json-patch+json' --request PATCH --data '[{'op': 'add', 'path': '/status/capacity/example.com~1dongle', 'value': '4'}]' http://localhost:8001/api/v1/nodes/gke-temp-c-default-pool-64035362-5089/status</code></p>",
        "image": ""
      },
      {
        "text": "<p><code>kubectl describe node <your-node-name></code></p>",
        "image": ""
      },
      {
        "text": "<p>Remove the resouce<code>curl --header 'Content-Type: application/json-patch+json' --request PATCH --data '[{'op': 'remove', 'path': '/status/capacity/example.com~1dongle'}]' http://localhost:8001/api/v1/nodes/gke-temp-c-default-pool-64035362-5089/status</code></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 2000,
    "date": "2020/01/14",
    "title": "Create Secrets",
    "itemList": [
      {
        "text": "<p>A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might otherwise be put in a Pod specification or in an image; putting it in a Secret object allows for more control over how it is used, and reduces the risk of accidental exposure.</p><p>Users can create secrets, and the system also creates some secrets. To use a secret, a pod needs to reference the secret. A secret can be used with a pod in two ways: as files in a volume mounted on one or more of its containers, or used by kubelet when pulling images for the pod.</p>",
        "image": ""
      },
      {
        "text": "<p>Secrets can be mounted as data volumes or be exposed as environment variables to be used by a container in a pod. They can also be used by other parts of the system, without being directly exposed to the pod. For example, they can hold credentials that other parts of the system should use to interact with external systems on your behalf.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Create files needed for rest of example: </b><ul><li><code>echo -n 'admin' > ./username.txt</code></li><li><code>echo -n '1f2d1e2e67df' > ./password.txt</code></li></ul></p>",
        "image": ""
      },
      {
        "text": "<p><b>The kubectl create secret command packages these files into a Secret and creates the object on the Apiserver: </b><code>kubectl create secret generic db-user-pass --from-file=./username.txt --from-file=./password.txt</code></p>",
        "image": ""
      },
      {
        "text": "<p><b>Check the secrets: </b><code>kubectl get secrets</code><br><code>kubectl describe secrets/db-user-pass</code></p>",
        "image": ""
      },
      {
        "text": "<p><b>Retrieved the secret: </b><code>kubectl get secret db-user-pass -o yaml</code></p>",
        "image": ""
      },
      {
        "text": "<p><b>Decode the password field: </b><code>echo 'MWYyZDFlMmU2N2Rm' | base64 --decode</code></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 3000,
    "date": "2020/04/05",
    "title": "Encrypting Secret Data at Rest",
    "itemList": [
      {
        "text": "<p>Generate a 32 byte random key and base64 encode it. If you’re on Linux or macOS, run the following command:<br><code>head -c 32 /dev/urandom | base64</code></p>",
        "image": ""
      },
      {
        "text": "<p>Place that value in the secret field.</p>",
        "image": ""
      },
      {
        "text": "<p>Set the --encryption-provider-config flag on the kube-apiserver to point to the location of the config file.</p>",
        "image": ""
      },
      {
        "text": "<p>Restart the API server.</p>",
        "image": ""
      }
    ]
  },
  {
    "id": 7000,
    "date": "2020/05/13",
    "title": "Others",
    "itemList": [
      {
        "text": "<p>verify the Kubelet version of your nodes:<br><code>kubectl get nodes -o=jsonpath=$'{range .items[*]}{@.metadata.name}: {@.status.nodeInfo.kubeletVersion}\\n{end}'</code></p>",
        "image": ""
      },
      {
        "text": "<p>Check whether the module is enabled:<br><code>cat /sys/module/apparmor/parameters/enabled</code></p>",
        "image": ""
      }
    ]
  }
]