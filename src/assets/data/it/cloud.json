[
  {
    "id": 100,
    "date": "2015/10/31",
    "title": "Cloud",
    "itemList": [
      {
        "text": "Cloud computing is a computing term or metaphor based on utility and consumption of computer resources.<p>Cloud computing involves deploying groups of remote servers and software networks that allow different kinds of data sources be uploaded for real time processing to generate computing results without the need to store processed data on the cloud.</p><p>Cloud computing relies on sharing of resources to achieve coherence and economies of scale, similar to a utility (like the electricity grid) over a network. At the foundation of cloud computing is the broader concept of converged infrastructure and shared services.",
        "image": ""
      },
      {
        "text": "<b>Service models</b><br/><ul><li>Infrastructure as a service (IaaS)</li><li>Platform as a service (PaaS)</li><li>Software as a service (SaaS)</li><li>Mobile 'backend' as a service (MBaaS)</li><li>Function as a service (FaaS)</li></ul>",
        "image": "../assets/image/it/cloud/cloudServiceModel.png",
        "imageClass": "mx-auto d-block"        
      },
      {
        "text": "<b>Deployment models</b><br/><ul><li>Private cloud</li><li>Public cloud</li><li>Hybrid cloud</li></ul>",
        "image": ""
      },
      {
        "text": "<b>Major Players in the Field</b><br/><ul><li><a href='http://aws.amazon.com' target='_blank'>AWS -  Amazon Web Services</a></li><li><a href='https://cloud.google.com' target='_blank'>Google Cloud Platform</a></li><li><a href='https://www.ibm.com/cloud/' target='_blank'>IBM Cloud</a></li><li><a href='https://cloud.oracle.com/home' target='_blank'>Oracle Cloud</a></li><li><a href='https://azure.microsoft.com/en-us/' target='_blank'>Microsoft Azure</a></li></ul>",
        "image": ""
      }
    ]
  },
  {
    "id": 200,
    "date": "2015/10/31",
    "title": "Cloud-Native",
    "itemList": [
      {
        "text": "<b>10 KEY ATTRIBUTES OF CLOUD-NATIVE APPLICATIONS</b>",
        "image": ""
      },
      {
        "text": "<ol><li><b>Packaged as lightweight containers: </b>Cloud-native applications are a collection of independent and autonomous services that are packaged as lightweight containers. Unlike virtual machines, containers can scale-out and scale-in rapidly. Since the unit of scaling shifts to containers, infrastructure utilization is optimized.</li><li><b>Developed with best-of-breed languages and frameworks: </b>Each service of a cloud-native application is developed using the language and framework best suited for the functionality. Cloud-native applications are polyglot; services use a variety of languages, runtimes and frameworks. For example, developers may build a real-time streaming service based on WebSockets, developed in Node.js, while choosing Python and Flask for exposing the API. The fine-grained approach to developing microservices lets them choose the best language and framework for a specific job.</li><li><b>Designed as loosely coupled microservices: </b>Services that belong to the same application discover each other through the application runtime. They exist independent of other services. Elastic infrastructure and application architectures, when integrated correctly, can be scaled-out with efficiency and high performance.</li><li><b>Centered around APIs for interaction and collaboration: </b>Cloud-native services use lightweight APIs that are based on protocols such as representational state transfer (REST), Google’s open source remote procedure call (gRPC) or NATS. REST is used as the lowest common denominator to expose APIs over hypertext transfer protocol (HTTP). For performance, gRPC is typically used for internal communication among services. NATS has publish-subscribe features which enable asynchronous communication within the application.</li><li><b>Architected with a clean separation of stateless and stateful services: </b>Services that are persistent and durable follow a different pattern that assures higher availability and resiliency. Stateless services exist independent of stateful services. There is a connection here to how storage plays into container usage. Persistence is a factor that has to be increasingly viewed in context with state, statelessness and — some would argue — micro-storage environments.</li><li><b>Isolated from server and operating system dependencies: </b>Cloud-native applications don’t have an affinity for any particular operating system or individual machine. They operate at a higher abstraction level. The only exception is when a microservice needs certain capabilities, including solid-state drives (SSDs) and graphics processing units (GPUs), that may be exclusively offered by a subset of machines.</li><li><b>Deployed on self-service, elastic, cloud infrastructure: </b>Cloud-native applications are deployed on virtual, shared and elastic infrastructure. They may align with the underlying infrastructure to dynamically grow and shrink — adjusting themselves to the varying load.</li><li><b>Managed through agile DevOps processes: </b>Each service of a cloud-native application goes through an independent life cycle, which is managed through an agile DevOps process. Multiple continuous integration/continuous delivery (CI/CD) pipelines may work in tandem to deploy and manage a cloud-native application.</li><li><b>Automated capabilities: </b>Cloud-native applications can be highly automated. They play well with the concept of infrastructure as code. Indeed, a certain level of automation is required simply to manage these large and complex applications.</li><li><b>Defined, policy-driven resource allocation: </b>Finally, cloud-native applications align with the governance model defined through a set of policies. They adhere to policies such as central processing unit (CPU) and storage quotas, and network policies that allocate resources to services. For example, in an enterprise scenario, central IT can define policies to allocate resources for each department. Developers and DevOps teams in each department have complete access and ownership to their share of resources.</li></ol>",
        "image": ""
      }
    ]
  },
  {
    "id": 300,
    "date": "2018/12/16",
    "title": "Docker",
    "itemList": [
      {
        "text": "<p>Docker is a computer program that performs operating-system-level virtualization, also known as 'containerization'.</p><p>Docker is used to run software packages called 'containers'. Containers are isolated from each other and bundle their own application, tools, libraries and configuration files; they can communicate with each other through well-defined channels. All containers are run by a single operating system kernel and are thus more lightweight than virtual machines. Containers are created from 'images' that specify their precise contents.</p>",
        "image": ""
      },
      {
        "text": "<b>Docker architecture</b><p>Docker uses a client-server architecture. The Docker client talks to the Docker daemon, which does the heavy lifting of building, running, and distributing your Docker containers. The Docker client and daemon can run on the same system, or you can connect a Docker client to a remote Docker daemon. The Docker client and daemon communicate using a REST API, over UNIX sockets or a network interface.</p>",
        "image": "../assets/image/it/cloud/dockerArchitecture.PNG"
      },
      {
        "text": "<b>Docker Daemon</b><p>The Docker daemon (dockerd) listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes. A daemon can also communicate with other daemons to manage Docker services. To run the daemon you type dockerd.</p><p>dockerd is the persistent process that manages containers. Docker uses different binaries for the daemon and client.</p><p>To run the daemon with debug output, use dockerd -D or add 'debug': true to the daemon.json file.</p>",
        "image": ""
      },
      {
        "text": "<b>Docker Client</b><p>The Docker client (docker) is the primary way that many Docker users interact with Docker. When you use commands such as docker run, the client sends these commands to dockerd, which carries them out. The docker command uses the Docker API. The Docker client can communicate with more than one daemon.</p>",
        "image": ""
      },
      {
        "text": "<b>Docker Image</b><p>An image is a read-only template with instructions for creating a Docker container. Often, an image is based on another image, with some additional customization. For example, you may build an image which is based on the ubuntu image, but installs the Apache web server and your application, as well as the configuration details needed to make your application run.</p><p>You might create your own images or you might only use those created by others and published in a registry. To build your own image, you create a Dockerfile with a simple syntax for defining the steps needed to create the image and run it. Each instruction in a Dockerfile creates a layer in the image. When you change the Dockerfile and rebuild the image, only those layers which have changed are rebuilt. This is part of what makes images so lightweight, small, and fast, when compared to other virtualization technologies.</p>",
        "image": ""
      },
      {
        "text": "<b>Docker Containers</b><p>A container is a runnable instance of an image. You can create, start, stop, move, or delete a container using the Docker API or CLI. You can connect a container to one or more networks, attach storage to it, or even create a new image based on its current state.</p><p>By default, a container is relatively well isolated from other containers and its host machine. You can control how isolated a container’s network, storage, or other underlying subsystems are from other containers or from the host machine.</p><p>A container is defined by its image as well as any configuration options you provide to it when you create or start it. When a container is removed, any changes to its state that are not stored in persistent storage disappear.</p>",
        "image": ""
      },
      {
        "text": "<b>Docker Registries</b><p>A Docker registry stores Docker images. Docker Hub is a public registry that anyone can use, and Docker is configured to look for images on Docker Hub by default. You can even run your own private registry. If you use Docker Datacenter (DDC), it includes Docker Trusted Registry (DTR).</p><p>When you use the docker pull or docker run commands, the required images are pulled from your configured registry. When you use the docker push command, your image is pushed to your configured registry.</p>",
        "image": ""
      },
      {
        "text": "<b>Dockerfile</b><p>Dockerfile is a text file that contains all commands, in order, needed to build a given image. A Dockerfile adheres to a specific format and set of instructions which you can find at <a href='https://docs.docker.com/engine/reference/builder/' target='_blank'>Dockerfile reference.</a></p><p>A Docker image consists of read-only layers each of which represents a Dockerfile instruction. The layers are stacked and each one is a delta of the changes from the previous layer.</p>",
        "image": ""
      },
      {
        "text": "<b>Docker Commands</b><br/><mark>docker info:</mark> Display docker information<br/><mark>docker build:</mark> Build an image from a Dockerfile<br/><mark>docker builder:</mark> Manage builds<br/><mark>docker create:</mark> Create a new container<br/><mark>docker container:</mark> Manage containers<br/><mark>docker ps:</mark> List containers<br/><mark>docker ps -a:</mark> List containers, including exited ones<br/><mark>docker image:</mark> Manage images<br/><mark>docker images:</mark> List images<br/><mark>docker rmi:</mark> remove one or more images (the dependent containers should be deleted first)<br/><mark>docker rm:</mark> remove one or more containers<br/><mark>docker container ls:</mark> List containers<br/><mark>docker run [name]:[tag/version]:</mark> run docker image<br/><mark>docker run -it [name]:[tag/version]:</mark> run in a interactive mode - input and prompt<br/><mark>docker run -p 80:5000:</mark> map host port 80 to docker port 5000<br/><mark>docker run -d:</mark> run container in backgroud<br/><mark>docker attach [id]:</mark> make backgrouded container run in front<br/><mark>docker stop [id]/[name]:</mark> stop a container<br/><mark>docker inspect [id]/[name]:</mark> Inspect the container<br/><mark>docker logs [id]/[name]:</mark> Display the logs<br/><mark>docker network ls:</mark> Display networks<br/><mark>docker pull:</mark> get the image from dockerhub to local<p>For more docker commands, please visit the following link: <a href='https://docs.docker.com/edge/engine/reference/commandline/docker/' target='_blank'>List Of Docker Commands</a></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 400,
    "date": "2018/12/26",
    "title": "Docker Compose",
    "itemList": [
      {
        "text": "<p>Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application’s services. Then, with a single command, you create and start all the services from your configuration.</p>",
        "image": ""
      },
      {
        "text": "<p>Using Compose is basically a three-step process:</p><ol><li>Define your app’s environment with a Dockerfile so it can be reproduced anywhere.</li><li>Define the services that make up your app in docker-compose.yml so they can be run together in an isolated environment.</li><li>Run docker-compose up and Compose starts and runs your entire app.</li></ol>",
        "image": ""
      },
      {
        "text": "<b>Features</b><p><ul><li>Multiple isolated environments on a single host</li><li>Preserve volume data when containers are created</li><li>Only recreate containers that have changed</li><li>Using variables and moving a composition between environments</li></ul></p>",
        "image": ""
      },
      {
        "text": "<b>Docker Compose Commands</b><p><mark>docker-compose build:</mark> Services are built once and then tagged, by default as project_service. For example, composetest_db. If the Compose file specifies an image name, the image is tagged with that name, substituting any variables beforehand. If you change a service’s Dockerfile or the contents of its build directory, run docker-compose build to rebuild it.</p><p><mark>docker-compose config:</mark> Validate and view the Compose file.</p><p><mark>docker-compose down:</mark> Stops containers and removes containers, networks, volumes, and images created by up. By default, the only things removed are: <ul><li>Containers for services defined in the Compose file</li><li>Networks defined in the networks section of the Compose file</li><li>The default network, if one is used</li></ul>Networks and volumes defined as external are never removed.</p><p><mark>docker-compose exec:</mark> This is the equivalent of docker exec. With this subcommand you can run arbitrary commands in your services. Commands are by default allocating a TTY, so you can use a command such as docker-compose exec web sh to get an interactive prompt.</p><p><mark>docker-compose run:</mark> Runs a one-time command against a service. For example, the following command starts the web service and runs bash as its command: <mark>docker-compose run web bash</mark></p><p><mark>docker-compose start [SERVICE]:</mark> Starts existing containers for a service.</p><p><mark>docker-compose stop [SERVICE]:</mark> Stops running containers without removing them. They can be started again with docker-compose start.</p><p><mark>docker-compose up:</mark> Builds, (re)creates, starts, and attaches to containers for a service. Unless they are already running, this command also starts any linked services.</p><p><mark>docker-compose ps:</mark> List containers</p><p><mark>docker service create --replicas=100 nodejs:</mark> create 100 nodejs containers</p><p>For more docker Compose commands, please visit the following link: <a href='https://docs.docker.com/compose/reference/' target='_blank'>List Of Docker Compose Commands</a></p>",
        "image": ""
      },
      {
        "text": "<b>To install docker compose in Ubuntu</b><p><code>sudo apt install docker-compose</code></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 500,
    "date": "2018/12/31",
    "title": "Docker Swarm",
    "itemList": [
      {
        "text": "<p>A swarm is a group of machines that are running Docker and joined into a cluster. After that has happened, you continue to run the Docker commands you’re used to, but now they are executed on a cluster by a swarm manager. The machines in a swarm can be physical or virtual. After joining a swarm, they are referred to as nodes.</p>",
        "image": ""
      },
      {
        "text": "<p>Docker swarm mode allows you to manage a cluster of Docker Engines, natively within the Docker platform. You can use the Docker CLI to create a swarm, deploy application services to a swarm, and manage swarm behavior.</p>",
        "image": ""
      },
      {
        "text": "<b>Docker Swarm Concepts</b><p><ul><li><b>Swarmkit</b> – a separate project which implements Docker’s orchestration layer and is used directly within Docker to implement Docker swarm mode.</li><li><b>Swarm</b> – a swarm consists of multiple Docker hosts which run in swarm mode and act as managers and workers.</li><li><b>Task</b> – the swarm manager distributes a specific number of tasks among the nodes, based on the service scale you specify. A task carries a Docker container and the commands to run inside the container. Once a task is assigned to a node, it cannot move to another node. It can only run on the assigned node or fail.</li><li><b>Service</b> – a service is the definition of the tasks to execute on the manager or worker nodes. When you create a service, you specify which container image to use and which commands to execute inside running containers. A key difference between services and standalone containers is that you can modify a service’s configuration, including the networks and volumes it is connected to, without manually restarting the service.</li><li><b>Nodes</b> – a swarm node is an individual Docker Engine participating in the swarm. You can run one or more nodes on a single physical computer or cloud server, but production swarm deployments typically include Docker nodes distributed across multiple machines.</li><li><b>Manager nodes</b> – dispatches units of work called tasks to worker nodes. Manager nodes also perform orchestration and cluster management functions.</li><li><b>Leader node</b> – manager nodes elect a single leader to conduct orchestration tasks, using the Raft consensus algorithm.</li><li><b>Worker nodes</b> – receive and execute tasks dispatched from manager nodes. By default manager nodes also run services as worker nodes. An agent runs on each worker node and reports on the tasks assigned to it to its manager node.</li><li><b>Load balancing</b> – the swarm manager uses ingress load balancing to expose the services running on the Docker swarm, enabling external access. The swarm manager assigns a configurable PublishedPort for the service. External components, such as cloud load balancers, can access the service on the PublishedPort of any node in the cluster, whether or not the node is currently running the task for the service. All nodes in the swarm route ingress connections to a running task instance. The swarm manager uses internal load balancing to distribute requests among services within the cluster based upon the DNS name of the service.</li></ul></p>",
        "image": ""
      },
      {
        "text": "<b>Docker Swarm Commands</b><p><mark>docker service create --replicas=100 nodejs:</mark> create 100 nodejs containers<br/><p>For more Kubernetes commands, please visit the following link: <a href='https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands' target='_blank'>List Of Kubernetes Commands</a></p>",
        "image": ""
      }    
    ]
  },
  {
    "id": 600,
    "date": "2018/12/26",
    "title": "Kubernetes",
    "itemList": [
      {
        "text": "<p>Kubernetes is a portable, extensible open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.</p><p>Kubernetes is not a traditional, all-inclusive PaaS (Platform as a Service) system. Since Kubernetes operates at the container level rather than at the hardware level, it provides some generally applicable features common to PaaS offerings, such as deployment, scaling, load balancing, logging, and monitoring. However, Kubernetes is not monolithic, and these default solutions are optional and pluggable. Kubernetes provides the building blocks for building developer platforms, but preserves user choice and flexibility where it is important.</p>",
        "image": ""
      },
      {
        "text": "<p>Google open-sourced the Kubernetes project in 2014. Kubernetes builds upon a decade and a half of experience that Google has with running production workloads at scale, combined with best-of-breed ideas and practices from the community.",
        "image": ""
      },
      {
        "text": "<b>Deploy a Wide Variety of Applications</b><p>Kubernetes Engine enables rapid application development and iteration by making it easy to deploy, update, and manage your applications and services. Kubernetes Engine isn't just for stateless applications either; you can attach persistent storage, and even run a database in your cluster. Simply describe the compute, memory, and storage resources your application containers require, and Kubernetes Engine provisions and manages the underlying cloud resources automatically. Support for hardware accelerators makes it easy to run Machine Learning, General Purpose GPU, High-Performance Computing, and other workloads that benefit from specialized hardware accelerators.",
        "image": ""
      },
      {
        "text": "<b>Operate Seamlessly with High Availability</b><p>Control your environment from the built-in Kubernetes Engine dashboard in Google Cloud console. Use routine health checks to detect and replace hung, or crashed, applications inside your deployments.Container replication strategies, monitoring, and automated repairs help ensure that your services are highly available and offer a seamless experience to your users. Google Site Reliability Engineers (SREs) constantly monitor your cluster and its compute, networking, and storage resources so you don't have to, giving you back time to focus on your applications.",
        "image": ""
      },       
      {
        "text": "<b>Scale Effortlessly to Meet Demand</b><p>Go from a single machine to thousands: Kubernetes Engine autoscaling allows you to handle increased user demand for your services, keeping them available when it matters most. Then, scale back in the quiet periods to save money, or schedule low-priority batch jobs to use up spare cycles. Kubernetes Engine helps you get the most out of your resource pool.",
        "image": ""
      },
      {
        "text": "<b>Run Securely on Google's Network</b><p>Connect to and isolate clusters no matter where you are with fine-grained network policies using Global Virtual Private Cloud (VPC) in Google Cloud. Use public services behind a single global anycast IP address for seamless load balancing. Protect against DOS and other types of edge attacks on your containers.",
        "image": ""
      }, 
      {
        "text": "<b>Move Freely between On-premises and Clouds</b><p>Kubernetes Engine runs Certified Kubernetes ensuring portability across clouds and on-premises. There's no vendor lock-in: you're free to take your applications out of Kubernetes Engine and run them anywhere Kubernetes is supported, including on your own on-premises servers. You can tailor integrations such as monitoring, logging, and CI/CD using Google Cloud Platform (GCP) and third party solutions in the ecosystem.",
        "image": ""
      },                                         
      {
        "text": "<b>Features</b><p><ul><li>a container platform</li><li>a microservices platform</li><li>a portable cloud platform and a lot more</li></ul></p>",
        "image": ""
      },
      {
        "text": "<b>Kubernetes Commands</b><p><mark>kubectl apply: </mark>Apply a configuration to a resource by filename or stdin. The resource name must be specified. This resource will be created if it doesn't exist yet. To use 'apply', always create the resource initially with either 'apply' or 'create --save-config'.</p><p><mark>kubectl config: </mark>Modify kubeconfig files using subcommands, such as 'delete-cluster', 'rename-context', 'set' etc. Some examples: <br/>&nbsp;&nbsp;&nbsp;&nbsp;<mark>kubectl config current-context</mark>: display the current-context my-context'.<br/>&nbsp;&nbsp;&nbsp;&nbsp;<mark>kubectl config delete-cluster minikube</mark>: Delete the minikube cluster</p><p><mark>kubectl create: </mark>Create a resource from a file or from stdin. JSON and YAML formats are accepted.</p><p><mark>kubectl delete: </mark>Delete resources by filenames, stdin, resources and names, or by resources and label selector. JSON and YAML formats are accepted.</p><p><mark>kubectl exec: </mark>Execute a command in a container.</p><p><mark>kubectl expose: </mark>Expose a resource as a new Kubernetes service.</p><p><mark>kubectl get: </mark>Display one or many resources.</p><p><mark>kubectl run: </mark>Create and run a particular image, possibly replicated.</p><p><mark>kubectl cluster-info: </mark>Display addresses of the master and services with label kubernetes.io/cluster-service=true To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.<br/><p>For more Kubernetes commands, please visit the following link: <a href='https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands' target='_blank'>List Of Kubernetes Commands</a></p>",
        "image": ""
      },
      {
        "text": "<b>Kubernetes Components</b>",
        "image": "../assets/image/it/cloud/k8s.png"
      },
      {
        "text": "",
        "image": "../assets/image/it/cloud/nodeOverview.png"
      },
      {
        "text": "Differences between kubectl apply and create",
        "image": "../assets/image/it/cloud/applyVScreate.png"
      }
    ]
  },
  {
    "id": 700,
    "date": "2015/11/3",
    "title": "Grid Computing",
    "itemList": [
      {
        "text": "Grid computing is the collection of computer resources from multiple locations to reach a common goal. The grid can be thought of as a distributed system with non-interactive workloads that involve a large number of files. Grid computing is distinguished from conventional high performance computing systems such as cluster computing in that grid computers have each node set to perform a different task/application. Grid computers also tend to be more heterogeneous and geographically dispersed (thus not physically coupled) than cluster computers. Although a single grid can be dedicated to a particular application, commonly a grid is used for a variety of purposes. Grids are often constructed with general-purpose grid middleware software libraries. Grid sizes can be quite large.",
        "image": ""
      },
      {
        "text": "<b>Major Players in the Field:</b>\n<ul><li><a href='http://en.wikipedia.org/wiki/Apache_Spark' target='_blank'>Apache Spark</a>: Apache Spark is an open-source cluster computing framework originally developed in the AMPLab at UC Berkeley. In contrast to Hadoop's two-stage disk-based MapReduce paradigm, Spark's in-memory primitives provide performance up to 100 times faster for certain applications. By allowing user programs to load data into a cluster's memory and query it repeatedly, Spark is well suited to machine learning algorithms.</li></ul>",
        "image": ""
      }
    ]
  },
  {
    "id": 800,
    "date": "2015/11/3",
    "title": "Big Data",
    "itemList": [
      {
        "text": "<p>Big data is a broad term for data sets so large or complex that traditional data processing applications are inadequate. Challenges include analysis, capture, data curation, search, sharing, storage, transfer, visualization, and information privacy. The term often refers simply to the use of predictive analytics or other certain advanced methods to extract value from data, and seldom to a particular size of data set.</p><p>Analysis of data sets can find new correlations, to 'spot business trends, prevent diseases, combat crime and so on.' Scientists, practitioners of media and advertising and governments alike regularly meet difficulties with large data sets in areas including Internet search, finance and business informatics. Scientists encounter limitations in e-Science work, including meteorology, genomics, connectomics, complex physics simulations, and biological and environmental research.</p>",
        "image": ""
      },
      {
        "text": "<b>Major Players in the Field:</b><br/><ul><li><a href='https://hadoop.apache.org' target='_blank'>Apache Hadoop</a>: Apache Hadoop is an open-source software framework written in Java for distributed storage and distributed processing of very large data sets on computer clusters built from commodity hardware. All the modules in Hadoop are designed with a fundamental assumption that hardware failures (of individual machines, or racks of machines) are commonplace and thus should be automatically handled in software by the framework.<br/><br/>The core of Apache Hadoop consists of a storage part (Hadoop Distributed File System (HDFS)) and a processing part (MapReduce). Hadoop splits files into large blocks and distributes them amongst the nodes in the cluster. To process the data, Hadoop MapReduce transfers packaged code for nodes to process in parallel, based on the data each node needs to process. This approach takes advantage of data localityâ€”nodes manipulating the data that they have on handâ€”to allow the data to be processed faster and more efficiently than it would be in a more conventional supercomputer architecture that relies on a parallel file system where computation and data are connected via high-speed networking.",
        "image": ""
      }
    ]
  },
  {
    "id": 900,
    "date": "2015/11/3",
    "title": "NoSQL",
    "itemList": [
      {
        "text": "A NoSQL (often interpreted as Not only SQL) database provides a mechanism for storage and retrieval of data that is modeled in means other than the tabular relations used in relational databases. Motivations for this approach include simplicity of design, horizontal scaling, and finer control over availability. The data structures used by NoSQL databases (e.g. key-value, graph, or document) differ from those used in relational databases, making some operations faster in NoSQL and others faster in relational databases. The particular suitability of a given NoSQL database depends on the problem it must solve.<p>NoSQL databases are increasingly used in big data and real-time web applications. NoSQL systems are also called 'Not only SQL' to emphasize that they may also support SQL-like query languages. Many NoSQL stores compromise consistency (in the sense of the CAP theorem) in favor of availability and partition tolerance. Barriers to the greater adoption of NoSQL stores include the use of low-level query languages, the lack of standardized interfaces, and huge investments in existing SQL. Most NoSQL stores lack true ACID transactions, although a few recent systems, such as FairCom c-treeACE, Google Spanner (though technically a NewSQL database), FoundationDB and OrientDB have made them central to their designs.",
        "image": ""
      },
      {
        "text": "<b>Types of NoSQL Databases (source: http://en.wikipedia.org/wiki/NoSQL):</b><br/><ul><li><b>Column:</b> Accumulo, Cassandra, Druid, HBase, Vertica</li><li><b>Document:</b> Lotus Notes, Clusterpoint, Apache CouchDB, Couchbase, MarkLogic, MongoDB, OrientDB, Qizx</li><li><b>Key-value:</b> CouchDB, Dynamo, FoundationDB, MemcacheDB, Redis, Riak, FairCom c-treeACE, Aerospike, OrientDB, MUMPS</li><li><b>Graph:</b> Allegro, Neo4J, InfiniteGraph, OrientDB, Virtuoso, Stardog</li><li><b>Multi-model:</b> OrientDB, FoundationDB, ArangoDB, Alchemy Database, CortexDB</li></ul>",
        "image": ""
      },
      {
        "text": "A more detailed classification is the following, based on one from Stephen Yen:",
        "image": "../assets/image/it/cloud/noSQLTypes.PNG"
      },
      {
        "text": "Performance (source: http://en.wikipedia.org/wiki/NoSQL):",
        "image": "../assets/image/it/cloud/noSQLPerformance.PNG"
      },
      {
        "text": "Major Players in the Document Oriented Database (source: http://en.wikipedia.org/wiki/NoSQL):",
        "image": "../assets/image/it/cloud/noSQLPlayers.PNG"
      },
      {
        "text": "<b>Redis:</b><br/><a href='http://redis.io/' target='_blank'>Redis</a>",
        "image": ""
      }
    ]
  },
  {
    "id": 1000,
    "date": "2019/02/16",
    "title": "MuleSoft",
    "itemList": [
      {
        "text": "<p>MuleSoft, Inc. is a software company that provides integration software for connecting applications, data and devices. Started in 2006, the company's Anypoint Platform of integration products is designed to integrate software as a service (SaaS), on-premises software, legacy systems, and more.</p>",
        "image": ""
      },
      {
        "text": "<p>The company originally provided middleware and messaging, and later expanded to provide an integration platform as a service (iPaaS) approach for companies through its main product, Anypoint Platform. MuleSoft's Anypoint Platform includes various components such as Anypoint Design Center, which allows API developers to design and build APIs; Anypoint Exchange, a library for API providers to share APIs, templates, and assets; and Anypoint Management Center, a centralized web interface to analyze, manage, and monitor APIs and integrations. MuleSoft also offers the Mule runtime engine, a runtime solution for connecting enterprise applications on-premises and to the cloud, designed to eliminate the need for custom point-to-point integration code.</p>",
        "image": ""
      },
      {
        "text": "<b>Design and Development Tools</b><br><p><ul><li>Anypoint Studio: An Eclipse-based graphical development environment for designing, testing and running Mule flows. It consists of two types of editors for development: Visual editor and XML editor.</li><li>Anypoint Enterprise Security: A suite of security-related features for secure access and transactions to Mule applications.</li><li>Mule Healthcare Toolkit: Provided to process HL7 standard messages used in healthcare organizations.</li><li>Mule IDE (now deprecated), A set of Eclipse plug-ins for developing, deploying and managing Mule projects.</li></ul></p>",
        "image": ""
      },
      {
        "text": "<b>Management Tools</b><br><p><ul><li>Mule Management Console: A user interface which provides run time management facility of deployment to the Mule Repository and clusters. Mule has an integration engine, but the community edition lacks the support for Advanced Management interfaces. MuleSoft offers an Enterprise Edition of Mule that provides a management console, a Service registry and higher availability.</li></ul></p>",
        "image": ""
      },
      {
        "text": "<b>Cloudhub</b><br><p><ul><li>Cloudhub is Mulesoft's Cloud-based integration platform for integration to connect apps, data and devices with integration connectors (like one to Twitter, etc.) platform as a service (iPaaS).</li></ul></p>",
        "image": ""
      },
      {
        "text": "<b>Mule ESB</b><br><p>Mule, the runtime engine of Anypoint Platform, is a lightweight Java-based enterprise service bus (ESB) and integration platform that allows developers to connect applications together quickly and easily, enabling them to exchange data. It enables easy integration of existing systems, regardless of the different technologies that the applications use, including JMS, Web Services, JDBC, HTTP, and more. The ESB can be deployed anywhere, can integrate and orchestrate events in real time or in batch, and has universal connectivity.</p>",
        "image": ""
      },
      {
        "text": "<p>The key advantage of an ESB is that it allows different applications to communicate with each other by acting as a transit system for carrying data between applications within your enterprise or across the Internet. Mule has powerful capabilities that include:<ul><li>Service creation and hosting — expose and host reusable services, using the ESB as a lightweight service container</li><li>Service mediation — shield services from message formats and protocols, separate business logic from messaging, and enable location-independent service calls</li><li>Message routing — route, filter, aggregate, and re-sequence messages based on content and rules</li><li>Data transformation — exchange data across varying formats and transport protocols</li></ul></p>",
        "image": "../assets/image/it/cloud/what-mule-esb.png",
        "imageClass": "mx-auto d-block"        
      }
    ]
  },
  {
    "id": 1100,
    "date": "2019/03/09",
    "title": "Azure Resource Manager",
    "itemList": [
      {
        "text": "<p>Azure Resource Manager enables you to repeatedly deploy your app and have confidence your resources are deployed in a consistent state. You define the infrastructure and dependencies for your app in a single declarative template. This template is flexible enough to use for all of your environments such as test, staging or production. If you create a solution from the Azure Marketplace, the solution will automatically include a template that you can use for your app.</p>",
        "image": ""
      },
      {
        "text": "<p>The Resource Manager and classic deployment models represent two different ways of deploying and managing your Azure solutions. You work with them through two different API sets, and the deployed resources can contain important differences. The two models are not compatible with each other. This article describes those differences.</p><p>To simplify the deployment and management of resources, Microsoft recommends that you use Resource Manager for all new resources. If possible, Microsoft recommends that you redeploy existing resources through Resource Manager.</p>",
        "image": ""
      },
      {
        "text": "<b>Terminology</b><br/><p><ul><li>Resource - A manageable item that is available through Azure. Virtual machines, storage accounts, web apps, databases, and virtual networks are examples of resources.</li><li>resource group - A container that holds related resources for an Azure solution. The resource group includes those resources that you want to manage as a group. You decide how to allocate resources to resource groups based on what makes the most sense for your organization. See Resource groups.</li><li>resource provider - A service that supplies Azure resources. For example, a common resource provider is Microsoft.Compute, which supplies the virtual machine resource. Microsoft.Storage is another common resource provider. See Resource providers.</li><li>Resource Manager template - A JavaScript Object Notation (JSON) file that defines one or more resources to deploy to a resource group or subscription. The template can be used to deploy the resources consistently and repeatedly. See Template deployment.</li><li>declarative syntax - Syntax that lets you state 'Here is what I intend to create' without having to write the sequence of programming commands to create it. The Resource Manager template is an example of declarative syntax. In the file, you define the properties for the infrastructure to deploy to Azure.</li></ul></p><p>Azure provides four levels of management scope: management groups, subscriptions, resource groups, and resources. Management groups are in a preview release. The following image shows an example of these layers.</p>",
        "image": "../assets/image/it/cloud/scope-levels.png",
        "imageClass": "mx-auto d-block"        
      }
    ]
  },
  {
    "id": 1200,
    "date": "2019/03/10",
    "title": "Azure DevOps Pipeline",
    "itemList": [
      {
        "text": "<b>Add gradle</b><br><p><ul><li>Pipeline->build->Edit->Tasks</li><li>On 'Phase x', 'Add a task to Phase x'</li><li>Search 'gradle'</li><li>Install or Add</li><li>Click the added task</li><li>Fill the required info</li></ul></p>",
        "image": ""
      },
      {
        "text": "<b>Add Tomcat Extension</b><br><p><ul><li>Pipeline->build->Edit->Tasks</li><li>On 'Phase x', 'Add a task to Phase x'</li><li>Search 'Apache Tomcat Deployment'</li><li>Install or Add</li><li>Click the added task</li><li>Fill the required info</li></ul></p>",
        "image": ""
      },
      {
        "text": "<b>Add JBoss and WildFly Extension</b><br><p><ul><li>Pipeline->build->Edit->Tasks</li><li>On 'Phase x', 'Add a task to Phase x'</li><li>Search 'JBoss'</li><li>Install or Add</li><li>Click the added task</li><li>Fill the required info</li></ul></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 1300,
    "date": "2019/10/20",
    "title": "GCP - Google Cloud Platform",
    "itemList": [
      {
        "text": "<b>GCP resources</b><br><p>GCP consists of a set of physical assets, such as computers and hard disk drives, and virtual resources, such as virtual machines (VMs), that are contained in Google's data centers around the globe. Each data center location is in a global region. Regions include Central US, Western Europe, and East Asia. Each region is a collection of zones, which are isolated from each other within the region. Each zone is identified by a name that combines a letter identifier with the name of the region. For example, zone a in the East Asia region is named asia-east1-a.</p><p>This distribution of resources provides several benefits, including redundancy in case of failure and reduced latency by locating resources closer to clients. This distribution also introduces some rules about how resources can be used together.</p>",
        "image": ""
      },
      {
        "text": "<b>Accessing resources through services</b><br><p>In cloud computing, what you might be used to thinking of as software and hardware products, become services. These services provide access to the underlying resources. The list of available GCP services is long, and it keeps growing. When you develop your website or application on GCP, you mix and match these services into combinations that provide the infrastructure you need, and then add your code to enable the scenarios you want to build.</p>",
        "image": ""
      },
      {
        "text": "<b>Global, regional, and zonal resources</b><br><p>Some resources can be accessed by any other resource, across regions and zones. These global resources include preconfigured disk images, disk snapshots, and networks. Some resources can be accessed only by resources that are located in the same region. These regional resources include static external IP addresses. Other resources can be accessed only by resources that are located in the same zone. These zonal resources include VM instances, their types, and disks.</p><p>The scope of an operation varies depending on what kind of resources you're working with. For example, creating a network is a global operation because a network is a global resource, while reserving an IP address is a regional operation because the address is a regional resource.</p><p>As you start to optimize your GCP applications, it's important to understand how these regions and zones interact. For example, even if you could, you wouldn't want to attach a disk in one region to a computer in a different region because the latency you'd introduce would make for very poor performance. Thankfully, GCP won't let you do that; disks can only be attached to computers in the same zone.</p>",
        "image": ""
      },
      {
        "text": "<b>Projects</b><br><p>Any GCP resources that you allocate and use must belong to a project. You can think of a project as the organizing entity for what you're building. A project is made up of the settings, permissions, and other metadata that describe your applications. Resources within a single project can work together easily, for example by communicating through an internal network, subject to the regions-and-zones rules. The resources that each project contains remain separate across project boundaries; you can only interconnect them through an external network connection.</p><p>Each GCP project has:<br><ul><li>A project name, which you provide.</li><li>A project ID, which you can provide or GCP can provide for you.</li><li>A project number, which GCP provides.</li></ul></p><p>A project serves as a namespace. This means every resource within each project must have a unique name, but you can usually reuse resource names if they are in separate projects. Some resource names must be globally unique. Refer to the documentation for the resource for details.</p>",
        "image": ""
      },
      {
        "text": "<b>Types of services</b><br><p><ul><li>Computing and hosting</li><li>Storage</li><li>Databases</li><li>Networking</li><li>Big data</li><li>Machine learning</li></ul></p>",
        "image": ""
      },
      {
        "text": "<b>Ways to interact with the services</b><br><p>GCP gives you three basic ways to interact with the services and resources.</p><p><ul><li>Google Cloud Platform Console</li><li>Command-line interface</li><li>Client libraries</li></ul></p>",
        "image": ""
      },
      {
        "text": "<b>GCP Compute services</b><br><p><ul><li>App Engine</li><li>Compute Engine</li><li>Kubernetes Engine</li><li>Cloud Function<p>Cloud Functions are serverless, single purpose and event driven solution. They can be also triggered on HTTP request. Each request/event are processed by a new instance of the function and provide a strong isolation between each invocation.</p><p>Only the code and the dependencies list are provided, the platform does all the rest: package, deploy and run the code.</p></li><li>Cloud Run<p>Cloud Run is build on top of Knative, that allows to serve stateless containers in a serverless way and it’s natively portable. Only stateless containers with HTTP endpoints are required to be run. There is no limitation in langages, binaries and dependencies.</p><p>By this way, the container has to integrate a webserver and can handle several requests in the same time and thus, can be multi-purpose.</p><p>Container packaging has to be done before deployment. Cloud Build can be used for this, or locally with Docker or other tools (like Kaniko or Jib)</p></li></ul></p>",
        "image": ""
      },
      {
        "text": "<b>Development tools and environments</b><br><p><ul><li>Cloud SDK<p>The Google Cloud SDK contains tools and libraries that enable you to easily create and manage resources on Cloud Platform, including App Engine, Compute Engine, Cloud Storage, BigQuery, Cloud SQL, and Cloud DNS. It runs on Windows, macOS, and Linux, and requires Python 2.7.x.</p></li><li>Cloud Shell<p>Google Cloud Shell makes it easy for you to manage your GCP Console projects and resources without having to install the Cloud SDK and other tools on your system. Cloud Shell runs on a temporary Compute Engine instance, so you can run the gcloud command-line tool and other utilities in your web browser.</p></li><li>Cloud Tools for IntelliJ, Eclipse, Visual Studio, PowerShell</li></ul></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 1500,
    "date": "2019/12/3",
    "title": "Spring Cloud",
    "itemList": [
      {
        "text": "<p>Spring Cloud provides tools for developers to quickly build some of the common patterns in distributed systems (e.g. configuration management, service discovery, circuit breakers, intelligent routing, micro-proxy, control bus, one-time tokens, global locks, leadership election, distributed sessions, cluster state). Coordination of distributed systems leads to boiler plate patterns, and using Spring Cloud developers can quickly stand up services and applications that implement those patterns. They will work well in any distributed environment, including the developer’s own laptop, bare metal data centres, and managed platforms such as Cloud Foundry.</p>",
        "image": ""
      },      
      {
        "text": "<b>Features</b><br><p>Spring Cloud focuses on providing good out of box experience for typical use cases and extensibility mechanism to cover others.<ul><li>Distributed/versioned configuration</li><li>Service registration and discovery</li><li>Routing</li><li>Service-to-service calls</li><li>Load balancing</li><li>Circuit Breakers</li><li>Global locks</li><li>Leadership election and cluster state</li><li>Distributed messaging</li></ul></p>",
        "image": ""
      },      
      {
        "text": "<p>Spring Cloud takes a very declarative approach, and often you get a lot of features with just a classpath change and/or an annotation. Example application that is a discovery client:</p>",
        "image": "../assets/image/it/cloud/springCloud.png"
      }
    ]
  },
  {
    "id": 1600,
    "date": "2019/12/4",
    "title": "Prometheus and Grafana on GCP",
    "itemList": [
      {
        "text": "<b>Overview</b><br><p>Prometheus is a monitoring toolkit. In this configuration, Prometheus collects the metrics from the Kubernetes cluster to which the application is deployed, and presents them in pre-configured Grafana dashboard. Additionally, you can configure the alerts using Prometheus Alert Manager.</p>",
        "image": ""
      },      
      {
        "text": "<b>Architecture</b>",
        "image": "../assets/image/it/cloud/prometheusGrafana.png"
      },      
      {
        "text": "<p>The application is designed to automatically collect metrics from Kubernetes cluster, collect them in the Prometheus server and present in Grafana. The application consists of the following components:</p>",
        "image": ""
      },      
      {
        "text": "<p><ul><li><b>Prometheus StatefulSet</b> - collects all the configured metrics by querying all the configured sources periodically. Each Prometheus Pod stores its data in a PersistentVolumeClaim.</li><li><b>Prometheus Node Exporter DaemonSet</b> - runs a Pod on each Kubernetes cluster node and collects metrics for the node's hardware and operating system by monitoring the host filesystem at /proc and /sys. The metrics are exposed on port 9100 of the Node Exporter's Pods.</li><li><b>Kube State Metrics Deployment</b> - listens to the Kubernetes API server and produces metrics related to resources (Deployments, Nodes, Pods, etc.). It exposes the metrics at /metrics on port 8080. Prometheus server consumes the metrics.</li><li><b>Prometheus Alert Manager</b> - receives the alerts raised by the Prometheus server and handles them accordingly to its configuration, specified in a ConfigMap.</li><li><b>Grafana StatefulSet</b> - provides a user interface for querying Prometheus about the metrics and visualizes the metrics in pre-configured dashboards.</li></ul></p>",
        "image": ""
      }
    ]
  },
  {
    "id": 1700,
    "date": "2020/07/05",
    "title": "Prometheus and Grafana on Azure",
    "itemList": [
      {
        "text": "<p>Azure Monitor for containers provides a seamless onboarding experience to collect Prometheus metrics. Typically, to use Prometheus, you need to set up and manage a Prometheus server with a store. By integrating with Azure Monitor, a Prometheus server is not required. You just need to expose the Prometheus metrics endpoint through your exporters or pods (application), and the containerized agent for Azure Monitor for containers can scrape the metrics for you.</p>",
        "image": "../assets/image/it/cloud/prometheusGrafanaOnAzure.png"
      }
    ]
  },
  {
    "id": 1800,
    "date": "2019/12/4",
    "title": "Grafana",
    "itemList": [
      {
        "text": "<p>Grafana allows you to query, visualize, alert on and understand your metrics no matter where they are stored. Create, explore, and share dashboards with your team and foster a data driven culture:</p>",
        "image": ""
      },
      {
        "text": "<ul><li><b>Visualize:</b> Fast and flexible client side graphs with a multitude of options. Panel plugins for many different way to visualize metrics and logs.</li><li><b>Dynamic Dashboards:</b> Create dynamic & reusable dashboards with template variables that appear as dropdowns at the top of the dashboard.</li><li><b>Explore Metrics:</b> Explore your data through ad-hoc queries and dynamic drilldown. Split view and compare different time ranges, queries and data sources side by side.</li><li><b>Explore Logs:</b> Experience the magic of switching from metrics to logs with preserved label filters. Quickly search through all your logs or streaming them live.</li><li><b>Alerting:</b> Visually define alert rules for your most important metrics. Grafana will continuously evaluate and send notifications to systems like Slack, PagerDuty, VictorOps, OpsGenie.</li><li><b>Mixed Data Sources:</b> Mix different data sources in the same graph! You can specify a data source on a per-query basis. This works for even custom datasources.</li></ul>",
        "image": ""
      }
    ]
  },
  {
    "id": 1900,
    "date": "2019/12/4",
    "title": "Elasticsearch",
    "itemList": [
      {
        "text": "<p>Elasticsearch is a search engine based on the Lucene library. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents. Elasticsearch is developed in Java. Following an open-core business model, parts of the software are licensed under various open-source licenses (mostly the Apache License), while other parts fall under the proprietary (source-available) Elastic License. Official clients are available in Java, .NET (C#), PHP, Python, Apache Groovy, Ruby and many other languages. According to the DB-Engines ranking, Elasticsearch is the most popular enterprise search engine followed by Apache Solr, also based on Lucene.</p>",
        "image": ""
      }
    ]
  },
  {
    "id": 2000,
    "date": "2019/12/4",
    "title": "Kibana",
    "itemList": [
      {
        "text": "<p>Kibana lets you visualize your Elasticsearch data and navigate the Elastic Stack so you can do anything from tracking query load to understanding the way requests flow through your apps.</p>",
        "image": ""
      }
    ]
  },
  {
    "id": 2100,
    "date": "2019/12/4",
    "title": "Logstash",
    "itemList": [
      {
        "text": "<p>Logstash is an open source, server-side data processing pipeline that ingests data from a multitude of sources simultaneously, transforms it, and then sends it to your favorite 'stash'.</p>",
        "image": ""
      }
    ]
  },
  {
    "id": 2200,
    "date": "2019/12/7",
    "title": "X-Pack",
    "itemList": [
      {
        "text": "<p>X-Pack is an Elastic Stack extension that provides security, alerting, monitoring, reporting, machine learning, and many other capabilities. By default, when you install Elasticsearch, X-Pack is installed.</p>",
        "image": ""
      }
    ]
  },
  {
    "id": 2300,
    "date": "2020/7/8",
    "title": "Kafka",
    "itemList": [
      {
        "text": "<p>Apache Kafka is an open-source stream-processing software platform developed by the Apache Software Foundation, written in Scala and Java. The project aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds.</p>",
        "image": ""
      },
      {
        "text": "<p><b>Publish</b></p>",
        "image": ""
      },
      {
        "text": "<p><b>Process</b></p>",
        "image": ""
      },
      {
        "text": "<p><b>Store</b></p>",
        "image": "../assets/image/it/cloud/kafka.PNG",
        "imageClass": "mx-auto d-block"        
      }
    ]
  }, 
  {
    "id": 5000,
    "date": "2019/12/8",
    "title": "Application Migration Strategies",
    "itemList": [
      {
        "text": "The 6 most common application migration strategies we see are (The 6 R’s):",
        "image": "../assets/image/it/cloud/appMigration.png"
      },
      {
        "text": "<p><ol><li>Rehosting — Otherwise known as 'lift-and-shift'</li><li>Replatforming — I sometimes call this 'lift-tinker-and-shift'</li><li>Repurchasing — Moving to a different product</li><li>Refactoring / Re-architecting — Re-imagining how the application is architected and developed, typically using cloud-native features</li><li>Retire — Get rid of</li><li>Retain — Usually this means “revisit” or do nothing (for now)</li></ol></p>",
        "image": ""
      }
    ]
  }
]